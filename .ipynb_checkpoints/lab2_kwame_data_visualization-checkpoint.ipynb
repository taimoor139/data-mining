{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 Data Visualization\n",
    "\n",
    "In this lab, you will be working with visualization techniques on several datasets, using different packages. The content covers the following topics:\n",
    "\n",
    "1. Grouped Bar Charts\n",
    "2. Parallel Coordinates\n",
    "3. Interactive plotting\n",
    "4. Plotting pitfalls with large data\n",
    "5. Plotting GEO data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing Packages\n",
    "!pip install pandas numpy scipy \n",
    "!pip install bokeh\n",
    "!pip install hvplot\n",
    "!pip install xarray holoviews datashader colorcet pyproj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bokeh plotting library\n",
    "\n",
    "In this lab, we will be using Bokeh library (https://docs.bokeh.org/) which is an interactive tool. You can use the toolbar on the chart to see the values for each bar, click and drag to zoom into a specific section, or click on the legend to hide/show a trace.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as s\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.layouts import row, column, gridplot\n",
    "from bokeh.models.widgets import Tabs, Panel\n",
    "\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-success' style=\"font-weight:bolder\">\n",
    "\n",
    "# § Part 1: Grouped Bar Charts with School earnings data\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/school_earnings.csv\")\n",
    "print(df.shape)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bar plot example\n",
    "\n",
    "Tooltips - tips/information for selected object/data points in the figure. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = figure(x_range=df.School, tooltips = [(\"(x,y)\", \"($x, $y)\")])\n",
    "fig.vbar(x=df.School, top=df.Gap, color='blue', width=0.75)\n",
    "show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding annotations with Label Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import LabelSet\n",
    "\n",
    "source1 = ColumnDataSource(data=df)\n",
    "fig = figure(x_range=df.School,plot_width=800, tooltips = [(\"School\", \"@School\"),(\"value\",\"@Gap\")])\n",
    "fig.vbar(x='School', top='Gap', color='green', width=0.75, source=source1)\n",
    "fig.xaxis.major_label_orientation = 1.0\n",
    "ls = LabelSet(x='School', y='Gap', text='Gap', x_offset=-10, y_offset=0, source=source1)\n",
    "fig.add_layout(ls)\n",
    "show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A more complex example - grouped bar plot comparing school earnings by genders, group (bars) by schools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = \"Women Men Gap\".split()\n",
    "groupsS = [(xx,yy) for xx in df.School for yy in columns]\n",
    "print(groupsS[:5])\n",
    "valuesS = df[columns].to_numpy().flatten()\n",
    "print(valuesS[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Example - a grouped bar plot comparing school earnings, grouped (bars) by university.\n",
    "from bokeh.transform import factor_cmap\n",
    "from bokeh.palettes import Set2_3\n",
    "from bokeh.models.ranges import FactorRange\n",
    "\n",
    "sourceS = ColumnDataSource(data=dict(names=groupsS, values=valuesS))\n",
    "fig = figure(x_range=FactorRange(*groupsS), plot_width=900)\n",
    "fig.vbar(x='names', top='values', width=0.8, source=sourceS,\n",
    "        line_color=\"white\", fill_color=factor_cmap('names', palette=Set2_3, factors=columns, start=1))\n",
    "\n",
    "fig.y_range.start = 0\n",
    "fig.x_range.range_padding = 0.1\n",
    "fig.xaxis.major_label_orientation = 1.5\n",
    "fig.xgrid.grid_line_color = None\n",
    "\n",
    "show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "### Task 1 - Produce a grouped bar plot comparing school earnings, group (bars) by genders.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-success' style=\"font-weight:bolder\">\n",
    "\n",
    "# § Part 2: Parallel Coordinates\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple hand-crafted example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.pandas\n",
    "\n",
    "d = {'A': [1, 2], 'B': [3, 1], 'C':[2,4], 'D': [4,2]}\n",
    "data = pd.DataFrame(data = d)\n",
    "\n",
    "class_label = ['c1', 'c2'] \n",
    "data['label'] = class_label \n",
    "\n",
    "data.head()\n",
    "\n",
    "hvplot.parallel_coordinates(data, class_column = 'label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "### Task 2\n",
    "\n",
    "### 2a - Add two more datapoints to the plot\n",
    "    \n",
    "### 2b - Add two more dimensions/features (for all samples)\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "### 2c - Produce a parallel coordinates plot for the Iris dataset (exclude feature \"Id\")\n",
    "    \n",
    "### 2d - By looking at the plot, figure out what is the range of petal_widths for setosa?\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = pd.read_csv('data/iris.csv')\n",
    "del iris['Id']\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-success' style=\"font-weight:bolder\">\n",
    "\n",
    "# § Part 3: Interactive plots with \"gapminder\" dataset.\n",
    "\n",
    "https://www.gapminder.org/videos/200-years-that-changed-the-world-bbc/    \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/gapminder.csv', thousands=',', index_col='Year')\n",
    "print(data.dtypes)\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Example countries: \\n \", data[\"Country\"].unique()[:10])\n",
    "print()\n",
    "print(\"Regions: \\n \", data[\"region\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[2005].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(height=400, x_axis_type='log',\n",
    "        x_range=(100, 100000), y_range=(0, 100))\n",
    "\n",
    "from bokeh.models import NumeralTickFormatter\n",
    "p.circle(x=data.loc[2005].income, y=data.loc[2005].life, color='pink')\n",
    "p.xaxis[0].formatter = NumeralTickFormatter(format='$0,')\n",
    "p.xaxis.axis_label = \"Income\"\n",
    "p.yaxis.axis_label = \"Life Expectancy\"\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bokeh.models import ColumnDataSource\n",
    "\n",
    "_yr_ = 2005\n",
    "\n",
    "source = ColumnDataSource(dict(\n",
    "    x=data.loc[_yr_].income,\n",
    "    y=data.loc[_yr_].life,\n",
    "    country=data.loc[_yr_].Country,\n",
    "    population=data.loc[_yr_].population,\n",
    "    region=data.loc[_yr_].region\n",
    "))\n",
    "source.data ## it is a dictionary\n",
    "\n",
    "## set of options\n",
    "PLOT_OPTS = dict(\n",
    "        height=400, x_axis_type='log',\n",
    "        x_range=(100, 100000), y_range=(0, 100),\n",
    ")\n",
    "\n",
    "p = figure(**PLOT_OPTS)\n",
    "p.circle(\n",
    "    x='x', y='y',\n",
    "    source=source,\n",
    "    color = 'pink'\n",
    ")\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import HoverTool, ResetTool, PanTool, WheelZoomTool\n",
    "hover = HoverTool(tooltips='@country', show_arrow=False)\n",
    "p = figure(\n",
    "    tools=[hover,ResetTool(),PanTool(),WheelZoomTool()],\n",
    "    **PLOT_OPTS)\n",
    "p.toolbar.active_scroll = p.select_one(WheelZoomTool)\n",
    "p.circle(\n",
    "    x='x', y='y',\n",
    "    color='pink',\n",
    "    source=source,\n",
    "    alpha=0.6, ## transparency\n",
    "    size = 20, ## changing the size of the circles\n",
    ")\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding more information to data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import (LinearInterpolator, CategoricalColorMapper)\n",
    "from bokeh.palettes import Spectral6\n",
    "\n",
    "size_mapper = LinearInterpolator(\n",
    "    x=[data.population.min(), data.population.max()],\n",
    "    y=[5, 50]\n",
    ")\n",
    "\n",
    "color_mapper = CategoricalColorMapper(\n",
    "    factors=list(data.region.unique()),\n",
    "    palette=Spectral6,\n",
    ")\n",
    "\n",
    "p = figure(\n",
    "    title=str(_yr_), toolbar_location='above',\n",
    "    tools=[hover],\n",
    "    **PLOT_OPTS)\n",
    "p.circle(\n",
    "    x='x', y='y',\n",
    "    size={'field': 'population', 'transform': size_mapper},\n",
    "    color={'field': 'region', 'transform': color_mapper},\n",
    "    alpha=0.6,\n",
    "    source=source,)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding 'region' as the legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(\n",
    "    title=str(_yr_), toolbar_location='above',\n",
    "    tools=[hover],\n",
    "    **PLOT_OPTS)\n",
    "p.circle(\n",
    "    x='x', y='y',\n",
    "    size={'field': 'population', 'transform': size_mapper},\n",
    "    color={'field': 'region', 'transform': color_mapper},\n",
    "    alpha=0.6,\n",
    "    source=source,\n",
    "    legend_field='region'\n",
    ")\n",
    "p.legend.border_line_color = None\n",
    "p.legend.location = (20, 0)\n",
    "p.right.append(p.legend[0])\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "## Task 3\n",
    "\n",
    "This is a picture of how the world looked like (Life Expectancy vs. GDP per Capita) in 2005... Are things (i.e. Life Expectancy and/or GDP per Capita) getting better? Make a similar plot for a few other years (hint: change 'the_year' variable... but be aware that the data is only available for some years)\n",
    " \n",
    "* How do you know if things are getting better or worse?\n",
    "* Is this universal, or only applies to a few countries?\n",
    "\n",
    "3a - Make a plot showing a trajectory of a country (of your choice from the year 1950 to 2015) to visualize its change in Life Expectancy and GDP per Capita, i.e. a plot where bubbles correspond to different years, instead of different countries. Try this for a few countries (at least one in each region). Can you also make information regarding times (the year) available in the plot?\n",
    "    \n",
    "3b - Make an interactive plot visualizing changes over the years (Hint: try \"interact\" from \"ipywidgets\" package)\n",
    "\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-success' style=\"font-weight:bolder\">\n",
    "\n",
    "# § Part 4: Common plotting pitfalls that get worse with large data\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Courtesy of https://anaconda.org/jbednar/plotting_pitfalls/notebook\n",
    "\n",
    "When working with large datasets, visualizations are often the only way available to understand the properties of that dataset -- there are simply too many data points to examine each one!  Thus it is very important to be aware of some common plotting problems that are minor inconveniences with small datasets but very serious problems with larger ones.\n",
    "\n",
    "We'll cover:\n",
    "\n",
    "1. [Overplotting](#1.-Overplotting)\n",
    "2. [Oversaturation](#2.-Oversaturation)\n",
    "3. [Undersampling](#3.-Undersampling)\n",
    "4. [Undersaturation](#4.-Undersaturation)\n",
    "5. [Underutilized range](#5.-Underutilized-range)\n",
    "6. [Nonuniform colormapping](#6.-Nonuniform-colormapping)\n",
    "\n",
    "You can [skip to the end](#Summary) if you just want to see an illustration of these problems.\n",
    "\n",
    "This notebook requires [HoloViews](http://holoviews.org), [colorcet](https://github.com/bokeh/colorcet), and matplotlib, and optionally scikit-image, which can be installed with:\n",
    "\n",
    "```\n",
    "conda install -c bokeh -c ioam holoviews colorcet matplotlib scikit-image\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```\n",
    "pip install bokeh ioam holoviews colorcet matplotlib scikit-image\n",
    "```\n",
    "\n",
    "We'll first load the plotting libraries and set up some defaults:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "import holoviews as hv\n",
    "hv.notebook_extension('matplotlib')\n",
    "\n",
    "%opts Points [color_index=2] (cmap=\"bwr\" edgecolors='k' s=50 alpha=1.0)\n",
    "%opts Scatter3D [color_index=3 fig_size=250] (cmap='bwr' edgecolor='k' s=50 alpha=1.0)\n",
    "%opts Image (cmap=\"gray_r\") {+axiswise}\n",
    "%opts RGB [bgcolor=\"black\" show_grid=False]\n",
    "\n",
    "import holoviews.plotting.mpl\n",
    "holoviews.plotting.mpl.MPLPlot.fig_alpha = 0\n",
    "holoviews.plotting.mpl.ElementPlot.bgcolor = 'white'\n",
    "\n",
    "from holoviews.operation.datashader import datashade\n",
    "from colorcet import fire\n",
    "datashade.cmap=fire[50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Overplotting\n",
    "\n",
    "Let's consider plotting some 2D data points that come from two separate categories, here plotted as blue and red in **A** and **B** below.  When the two categories are overlaid, the appearance of the result can be very different depending on which one is plotted first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blues_reds(offset=0.5,pts=300):\n",
    "    blues = (np.random.normal( offset,size=pts), np.random.normal( offset,size=pts), -1*np.ones((pts)))\n",
    "    reds  = (np.random.normal(-offset,size=pts), np.random.normal(-offset,size=pts),  1*np.ones((pts)))\n",
    "    return hv.Points(blues, vdims=['c']), hv.Points(reds, vdims=['c'])\n",
    "blues,reds = blues_reds()\n",
    "blues + reds + reds*blues + blues*reds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots **C** and **D** shown the same distribution of points, yet they give a very different impression of which category is more common, which can lead to incorrect decisions based on this data.  Of course, both are equally common in this case.  The cause for this problem is simply occlusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Occlusion of data by other data is called **overplotting** or **overdrawing**, and it occurs whenever a datapoint or curve is plotted on top of another datapoint or curve, obscuring it.  It's thus a problem not just for scatterplots, as here, but for curve plots, 3D surface plots, 3D bar graphs, and any other plot type where data can be obscured.\n",
    "\n",
    "\n",
    "### 2. Oversaturation\n",
    "\n",
    "You can reduce problems with overplotting by using transparency/opacity, via the alpha parameter provided to control opacity in most plotting programs.  E.g. if alpha is 0.1, full color saturation will be achieved only when 10 points overlap, reducing the effects of plot ordering but making it harder to see individual points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts Points (s=50 alpha=0.1)\n",
    "blues + reds + reds*blues + blues*reds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "### Task 4a\n",
    "\n",
    "Try several different values of alpha/transparency parameter, and observe how the plots change.\n",
    "\n",
    "What is the best value of alpha?\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here **C** and **D** look very similar (as they should, since the distributions are identical), but there are still a few locations with **oversaturation**, a problem that will occur when more than 10 points overlap. In this example the oversaturated points are located near the middle of the plot, but the only way to know whether they are there would be to plot both versions and compare, or to examine the pixel values to see if any have reached full saturation (a necessary but not sufficient condition for oversaturation).  Locations where saturation has been reached have problems similar to overplotting, because only the last 10 points plotted will affect the final color (for alpha of 0.1).\n",
    "\n",
    "Worse, even if one has set the alpha value to approximately or usually avoid oversaturation, as in the plot above, the correct value depends on the dataset.  If there are more points overlapping in that particular region, a manually adjusted alpha setting that worked well for a previous dataset will systematically misrepresent the new dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts Points (alpha=0.1)\n",
    "blues,reds = blues_reds(pts=600)\n",
    "blues + reds + reds*blues + blues*reds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "### Task 4b\n",
    "\n",
    "Again, try several different values of alpha/transparency parameter, and observe how the plots change.\n",
    "\n",
    "What is the best value of alpha in this case? Is it the same as above? Why?\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it even more complicated, the correct alpha also depends on the dot size, because smaller dots have less overlap for the same dataset. With smaller dots, **C** and **D** look more similar, but the color of the dots is now difficult to see in all cases because the dots are too transparent for this size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts Points (s=10 alpha=0.1 edgecolor=None)\n",
    "blues + reds + reds*blues + blues*reds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it is very difficult to find settings for the dotsize and alpha parameters that correctly reveal the data, even for relatively small and obvious datasets like these.  With larger datasets with unknown contents, it is difficult to detect that such problems are occuring, leading to false conclusions based on inappropriately visualized data.\n",
    "\n",
    "### 3. Undersampling\n",
    "\n",
    "With a single category instead of the multiple categories shown above, oversaturation simply obscures spatial differences in density.  For instance, 10, 20, and 2000 single-category points overlapping will all look the same visually, for alpha=0.1.  Let's again consider an example that has a sum of two normal distributions slightly offset from one another, but no longer using color to separate them into categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts Points.Small_dots (s=1 alpha=1 c='b') Points.Tiny_dots (s=0.1 alpha=0.1)\n",
    "\n",
    "def gaussians(specs=[(1.5,0,1.0),(-1.5,0,1.0)],num=100):\n",
    "    \"\"\"\n",
    "    A concatenated list of points taken from 2D Gaussian distributions.\n",
    "    Each distribution is specified as a tuple (x,y,s), where x,y is the mean\n",
    "    and s is the standard deviation.  Defaults to two horizontally\n",
    "    offset unit-mean Gaussians.\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    dists = [(np.random.normal(x,s,num), np.random.normal(y,s,num)) for x,y,s in specs]\n",
    "    return np.hstack([d[0] for d in dists]), np.hstack([d[1] for d in dists])\n",
    "    \n",
    "hv.Points(gaussians(num=600),   label=\"600 points\",   group=\"Small dots\") + \\\n",
    "hv.Points(gaussians(num=60000), label=\"60000 points\", group=\"Small dots\") + \\\n",
    "hv.Points(gaussians(num=600),   label=\"600 points\",   group=\"Tiny dots\")  + \\\n",
    "hv.Points(gaussians(num=60000), label=\"60000 points\", group=\"Tiny dots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as shown for the multiple-category case above, finding settings to avoid overplotting and oversaturation is difficult.  The \"Small dots\" setting (size 0.1, full alpha) works fairly well for a sample of 600 points **A**, but it has serious overplotting issues for larger datasets, obscuring the shape and density of the distribution **B**.  Using the \"Tiny dots\" setting (10 times smaller dots, alpha 0.1) works well for the larger dataset **D**, but not at all for the 600-point dataset **C**.  Clearly, not all of these settings are accurately conveying the underlying distribution, as they all appear quite different from one another. Similar problems occur for the same size of dataset, but with greater or lesser levels of overlap between points, which of course varies with every new dataset.  \n",
    "\n",
    "In any case, as dataset size increases, at some point plotting a full scatterplot like any of these will become impractical with current plotting software.  At this point, people often simply subsample their dataset, plotting 10,000 or perhaps 100,000 randomly selected datapoints.  But as panel **A** shows, the shape of an **undersampled** distribution can be very difficult or impossible to make out, leading to incorrect conclusions about the distribution.  Such problems can occur even when taking very large numbers of samples, if examining sparsely populated regions of the space, which will approximate panel **A** for some plot settings and panel **C** for others.  The actual shape of the distribution is only visible if sufficient datapoints are available in that region *and* appropriate plot settings are used, as in **D**, but ensuring that both conditions are true is a quite difficult process of trial and error, making it very likely that important features of the dataset will be missed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "### Task 4c\n",
    "\n",
    "Add a third normal distribution (gaussian) to the plot.\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid undersampling large datasets, researchers often use 2D histograms visualized as heatmaps, rather than scatterplots showing individual points.  A heatmap has a fixed-size grid regardless of the dataset size, so that they can make use of all the data.  Heatmaps effectively approximate a probability density function over the specified space, with coarser heatmaps averaging out noise or irrelevant variations to reveal an underlying distribution, and finer heatmaps able to represent more details in the distribution.\n",
    "\n",
    "Let's look at some heatmaps with different numbers of bins for the same two-Gaussians distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap(coords,bins=10,offset=0.0,transform=lambda d,m:d, label=None):\n",
    "    \"\"\"\n",
    "    Given a set of coordinates, bins them into a 2d histogram grid\n",
    "    of the specified size, and optionally transforms the counts\n",
    "    and/or compresses them into a visible range starting at a \n",
    "    specified offset between 0 and 1.0.\n",
    "    \"\"\"\n",
    "    hist,xs,ys  = np.histogram2d(coords[0], coords[1], bins=bins)\n",
    "    counts      = hist[:,::-1].T\n",
    "    transformed = transform(counts,counts!=0)\n",
    "    span        = transformed.max()-transformed.min()\n",
    "    compressed  = np.where(counts!=0,offset+(1.0-offset)*transformed/span,0)\n",
    "    args        = dict(label=label) if label else {}\n",
    "    return hv.Image(compressed,bounds=(xs[-1],ys[-1],xs[1],ys[1]),**args)\n",
    "\n",
    "hv.Layout([heatmap(gaussians(num=60000),bins) for bins in [8,14,20,60,200]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, a too-coarse binning grid **A** cannot represent this distribution faithfully, but with enough bins **C**, the heatmap will approximate a tiny-dot scatterplot like plot **D** in the previous figure.  For intermediate grid sizes **B** the heatmap can average out the effects of undersampling; **B** is actually a more faithful representation of the *distribution* than **C** is (which we know is two offset 2D Gaussians), while **C** more faithfully represents the *sampling* (i.e., the individual points drawn from this distribution).  Thus choosing a good binning grid size for a heatmap does take some expertise and knowledge of the goals of the visualization, and it's always useful to look at multiple binning-grid spacings for comparison.  Still, at least the binning parameter is something meaningful at the data level (how coarse a view of the data is desired?) rather than just a plotting detail (what size and transparency should I use for the points?) that must be determined arbitrarily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "### Task 4d\n",
    "\n",
    "Add two more grid sizes, one between \"A\" and \"B\", and one between \"B\" and \"C\".\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any case, at least in principle, the heatmap approach can entirely avoid the first three problems above: overplotting (since multiple data points sum arithmetically into the grid cell, without obscuring one another), oversaturation (because the minimum and maximum counts observed can automatically be mapped to the two ends of a visible color range), and undersampling (since the resulting plot size is independent of the number of data points, allowing it to use an unbounded amount of incoming data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Undersaturation\n",
    "\n",
    "Of course, heatmaps come with their own plotting pitfalls.  One rarely appreciated issue common to both heatmaps and alpha-based scatterplots is **undersaturation**, where large numbers of data points can be missed entirely because they are spread over many different heatmap bins or many nearly transparent scatter points.  To look at this problem, let's again consider a set of multiple 2D Gaussians, but this time with different amounts of spread (standard deviation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = gaussians(specs=[(2,2,0.02), (2,-2,0.1), (-2,-2,0.5), (-2,2,1.0), (0,0,3)],num=10000)\n",
    "hv.Points(dist).opts(style=dict(alpha=0.02)) + hv.Points(dist).opts(style=dict(s=0.1)) + hv.Points(dist).opts(style=dict(s=0.05,alpha=0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots **A**, **B**, and **C** are all scatterplots for the same data, which is a sum of 5 Gaussian distributions at different locations and with different standard deviations:\n",
    "\n",
    "1. Location   (2,2):  very narrow spread\n",
    "2. Location  (2,-2): narrow spread\n",
    "3. Location (-2,-2): medium spread\n",
    "4. Location  (-2,2): large spread\n",
    "5. Location   (0,0): very large spread\n",
    "\n",
    "In plot **A**, of course, the very large spread covers up everything else, completely obscuring the structure of this dataset by overplotting.  Plots **B** and **C** reveal the structure better, but they required hand tuning and neither one is particularly satisfactory.  In **B** there are four clearly visible Gaussians, but all but the largest appear to have the same density of points per pixel, which we know is not the case from how the dataset was constructed, and the smallest is nearly invisible.  Each of the five Gaussians has the same number of data points (10000), but the second-largest looks like it has more than the others, and the narrowest one is likely to be overlooked altogether, which is thus a clear example of oversaturation obscuring important features.  Yet if we try to combat the oversaturation by using transparency in **C**, we now get a clear problem with **undersaturation** -- the \"very large spread\" Gaussian is now essentially invisible.  Again, there are just as many datapoints in that category, but we'd never even know they were there if only looking at **C**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "### Task 4e\n",
    "\n",
    "Tune the parameters of the plots (point size, transparency, etc.) to improve the results in each of the three plots.\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar problems occur for a heatmap view of the same data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.Layout([heatmap(dist,bins) for bins in [8,20,200]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the narrow-spread distributions lead to pixels with a very high count, and if the other pixels are linearly ramped into the available color range, from zero to that high count value, then the wider-spread values are obscured (as in **B**) or entirely invisible (as in **C**). \n",
    "\n",
    "To avoid undersaturation, you can add an offset to ensure that low-count (but nonzero) bins are mapped into a visible color, with the remaining intensity scale used to indicate differences in counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.Layout([heatmap(dist,bins,offset=0.2) for bins in [8,20,200]]).cols(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such mapping entirely avoids undersaturation, since all pixels are either clearly zero (in the background color, i.e. white in this case), or a non-background color taken from the colormap.   The widest-spread Gaussian is now clearly visible in all cases.  \n",
    "\n",
    "However, the actual structure (5 Gaussians of different spreads) is still not visible.  In **A** the problem is clearly too-coarse binning, but in **B** the binning is also somewhat too coarse for this data, since the \"very narrow spread\" and \"narrow spread\" Gaussians show up identically, each mapping entirely into a single bin (the two black pixels).  **C** shouldn't suffer from too-coarse binning, yet it still looks more like a plot of the \"very large spread\" distribution alone, than a plot of these five distributions of different spreads, and it is thus still highly misleading despite the correction for undersaturation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "### Task 4f\n",
    "\n",
    "Tune the parameters of the plots (point size, transparency, etc.) to improve the results in each of the three plots.\n",
    "\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Underutilized range\n",
    "\n",
    "So, what is the problem in plot **C** above?  By construction, we've avoided the first four pitfalls: **overplotting**, **oversaturation**, **undersampling**, and **undersaturation**.  But the problem is now more subtle: differences in datapoint density are not visible between the five Gaussians, because all or nearly all pixels end up being mapped into either the bottom end of the visible range (light gray), or the top end (black, used only for the single pixel holding the \"very narrow spread\" distribution). The entire rest of the visible colors in this gray colormap are unused, conveying no information to the viewer about the rich structure that we know this distribution contains.  If the data were uniformly distributed over the range from minimum to maximum counts per pixel (0 to 10,000, in this case), then the above plot would work well, but that's not the case for this dataset or for most real-world datasets.\n",
    "\n",
    "So, let's try transforming the data from its default linear representation (integer count values) into something that preserves relative differences in count values but maps them into visually distinct colors.  A logarithmic transformation is one common choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.Layout([heatmap(dist,bins,offset=0.2,transform=lambda d,m: np.where(m,np.log1p(d),0)) for bins in [8,20,200]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha!  We can now see the full structure of the dataset, with all five Gaussians clearly visible in **B** and **C**, and the relative spreads also clearly visible in **C**.  \n",
    "\n",
    "We still have a problem, though.  The choice of a logarithmic transform was fairly arbitrary, and it mainly works well because we happened to have used an approximately geometric progression of spread sizes when constructing the example.  For large datasets with truly unknown structure, can we have a more principled approach to mapping the dataset values into a visible range?  \n",
    "\n",
    "Yes, if we think of the visualization problem in a different way.  The underlying difficulty in plotting this dataset (as for very many real-world datasets) is that the values in each bin are numerically very different (ranging from 10,000, in the bin for the \"very narrow spread\" Gaussian, to 1 (for single datapoints from the \"very large spread\" Gaussian)).  Given the 256 gray levels available in a normal monitor (and the similarly limited human ability to detect differences in gray values), numerically mapping the data values into the visible range is not going to work well.  But given that we are already backing off from a direct numerical mapping in the above approaches for correcting undersaturation and for doing log transformations, what if we entirely abandon the numerical mapping approach, using the numbers only to form a partial ordering of the data values?  Such an approach would be a rank-order plot, preserving order and not magnitudes.  For 100 gray values, you can think of it as a percentile-based plot, with the lowest 1% of the data values mapping to the first visible gray value, the next 1% mapping to the next visible gray value, and so on to the top 1% of the data values mapping to the gray value 255 (black in this case).  The actual data values would be ignored in such plots, but their relative magnitudes would still determine how they map onto colors on the screen, preserving the structure of the distribution rather than the numerical values.\n",
    "\n",
    "We can approximate such a rank-order or percentile encoding using the histogram equalization function from an image-processing package, which makes sure that each gray level is used for about the same number of pixels in the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from skimage.exposure import equalize_hist\n",
    "    eq_hist = lambda d,m: equalize_hist(1000*d,nbins=100000,mask=m)\n",
    "except ImportError:\n",
    "    eq_hist = lambda d,m: d\n",
    "    print(\"scikit-image not installed; skipping histogram equalization\")\n",
    "    \n",
    "hv.Layout([heatmap(dist,bins,transform=eq_hist) for bins in [8,20,200]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot **C** now reveals the full structure that we know was in this dataset, i.e. five Gaussians with different spreads, with no arbitrary parameter choices. (Well, there is a \"number of bins\" parameter for building the histogram for equalizing, but for integer data like this even that parameter can be eliminated entirely.)  The differences in counts between pixels are now very clearly visible, across the full (and very wide) range of counts in the original data.\n",
    "\n",
    "Of course, we've lost the actual counts themselves, and so we can no longer tell just how many datapoints are in the \"very narrow spread\" pixel in this case.  So plot **C** is accurately conveying the structure, but additional information would need to be provided to show the actual counts, by adding a color key mapping from the visible gray values into the actual counts and/or by providing hovering value information.\n",
    "\n",
    "At this point, one could also consider explicitly highlighting hotspots so that they cannot be overlooked.  In plots B and C above, the two highest-density pixels are mapped to the two darkest pixel colors, which can reveal problems with your monitor settings if they were adjusted to make dark text appear blacker.  Thus on those monitors, the highest values may not be  clearly distinguishable from each other or from nearby grey values, which is a possible downside to fully utilizing the dynamic range available.  But once the data is reliably and automatically mapped into a repeatable, reliable, fully utilized range for display, making explicit adjustments (e.g. based on wanting to make hotspots particularly clear) can be done in a principled way that doesn't depend on the actual data distribution (e.g. by just making the top few pixel values into a different color, or by stretching out those portions of the color map to show the extremes more safely across different monitors). Before getting into such specialized manipulations, there's a big pitfall to avoid first:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Nonuniform colormapping\n",
    "\n",
    "Let's say you've managed avoid pitfalls 1-5 somehow. However, there is one more problem waiting to catch you at the last stage, ruining all of your work eliminating the other issues: using a perceptually non-uniform colormap. A heatmap requires a colormap before it can be visualized, i.e., a lookup table from a data value (typically a normalized magnitude in the range 0 to 1) to a pixel color. The goal of a scientific visualization is to reveal the underlying properties of the data to your visual system, and to do so it is necessary to choose colors for each pixel that lead the viewer to perceive that data faithfully. Unfortunately, most of the colormaps in common use in plotting programs are highly nonuniform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorcet\n",
    "hv.Layout([heatmap(dist,200,transform=eq_hist,label=cmap)(style=dict(cmap=cmap)) for cmap in [\"hot\",\"cet_fire\"]]).cols(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing A to B it should be clear that the \"cet_fire\" colormap is revealing much more of the data, accurately rendering the density differences between each of the different blobs. The unsuitable \"hot\" colormap is mapping all of the high density regions to perceptually indistinguishable shades of bright yellow/white, giving an \"oversaturated\" appearance even though we know the underlying heatmap array is not oversaturated (by construction). Luckily it is easy to avoid this problem; just use one of the 50 perceptually uniform colormaps available in the colorcet package, one of the four shipped with matplotlib (viridis, plasma, inferno, or magma), or the Parula colormap shipped with Matlab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Summary\n",
    "\n",
    "Starting with plots of specific datapoints, we showed how typical visualization techniques will systematically misrepresent the distribution of those points.  Here's an example of each of those six problems, all for the same distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts Layout [sublabel_format=\"\" tight=True] Points {-axiswise}\n",
    "(hv.Points(dist,label=\"1. Overplotting\") + \n",
    " hv.Points(dist,label=\"2. Oversaturation\").opts(style=dict(s=0.1,alpha=0.5)) + \n",
    " hv.Points((dist[0][::200],dist[1][::200]),label=\"3. Undersampling\").opts(style=dict(s=2,alpha=0.5)) + \n",
    " hv.Points(dist,label=\"4. Undersaturation\").opts(style=dict(s=0.01,alpha=0.05)) + \n",
    " heatmap(dist,200,offset=0.2,label=\"5. Underutilized dynamic range\") +\n",
    " heatmap(dist,200,transform=eq_hist,label=\"6. Nonuniform colormapping\").opts(style=dict(cmap=\"hot\"))).cols(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we could avoid each of these problems by hand, using trial and error based on our knowledge about the underlying dataset, since we created it.  But for big data in general, these issues are major problems, because you don't know what the data *should* look like. Thus:\n",
    "\n",
    "#### For big data, you don't know when the viz is lying\n",
    "\n",
    "I.e., visualization is supposed to help you explore and understand your data, but if your visualizations are systematically misrepresenting your data because of **overplotting**, **oversaturation**, **undersampling**, **undersaturation**, **underutilized range**, and **nonuniform colormapping**, then you won't be able to discover the real qualities of your data and will be unable to make the right decisions.\n",
    "\n",
    "Luckily, using the systematic approach outlined in this discussion, you can avoid *all* of these pitfalls, allowing you to render your data faithfully without requiring *any* \"magic parameters\" that depend on your dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Datashader](https://github.com/bokeh/datashader)\n",
    "\n",
    "The steps above show how to avoid the six main plotting pitfalls by hand, but it can be awkward and relatively slow to do so.  Luckily there is a new Python library available to automate and optimize these steps, named [Datashader](https://github.com/bokeh/datashader).  Datashader avoids users having to make dataset-dependent decisions and parameter settings when visualizing a new dataset.  Datashader makes it practical to create accurate visualizations of datasets too large to understand directly, up to a billion points on a normal laptop and larger datasets on a compute cluster.  As a simple teaser, the above steps can be expressed very concisely using the Datashader interface provided by [HoloViews](http://holoviews.org):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%output size=200\n",
    "\n",
    "datashade(hv.Points(dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = gaussians(specs=[(2,2,0.02), (2,-2,0.1), (-2,-2,0.5), (-2,2,1.0), (0,0,3)],num=10000000)\n",
    "\n",
    "datashade(hv.Points(dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-success' style=\"font-weight:bolder\">\n",
    "\n",
    "# § Part 5: [Visualizing the Taxi Trajectory dataset](https://archive.ics.uci.edu/ml/datasets/Taxi+Service+Trajectory+-+Prediction+Challenge,+ECML+PKDD+2015)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains taxi trajectories performed by 442 taxis running in the city of Porto, in Portugal (from 01/07/2013 to 30/06/2014). These taxis operate through a taxi dispatch central, using mobile data terminals installed in the vehicles. Each ride were categorized into three categories: A) taxi central based, B) stand-based or C) non-taxi central based. For the first, an anonymized id is provided, when such information is available from the telephone call. The last two categories refer to services that were demanded directly to the taxi drivers on a B) taxi stand or on a C) random street. The dataset is used in ECML/PKDD 15: Taxi Trajectory Prediction, where the goal is to predict the destination of taxi trajectories in the city of Porto, Portugal, with maximum accuracy. \n",
    "\n",
    "Each data sample corresponds to one completed trip. It contains a total of 9 (nine) features, described as follows:\n",
    "\n",
    "TRIP_ID: (String) It contains a unique identifier for each trip;\n",
    "\n",
    "CALL_TYPE: (char) It identifies the way used to demand this service. It may contain one of three possible values:\n",
    "- 'A' if this trip was dispatched from the central;\n",
    "- 'B' if this trip was demanded directly to a taxi driver at a specific stand;\n",
    "- 'C' otherwise (i.e. a trip demanded on a random street).\n",
    "\n",
    "ORIGIN_CALL: (integer) It contains a unique identifier for each phone number which was used to demand, at least, one service. It identifies the trip's customer if CALL_TYPE='A'. Otherwise, it assumes a NULL value;\n",
    "\n",
    "ORIGIN_STAND: (integer): It contains a unique identifier for the taxi stand. It identifies the starting point of the trip if CALL_TYPE='B'. Otherwise, it assumes a NULL value;\n",
    "\n",
    "TAXI_ID: (integer): It contains a unique identifier for the taxi driver that performed each trip;\n",
    "\n",
    "TIMESTAMP: (integer) Unix Timestamp (in seconds). It identifies the trip's start;\n",
    "\n",
    "DAYTYPE: (char) It identifies the daytype of the trip's start. It assumes one of three possible values:\n",
    "- 'B' if this trip started on a holiday or any other special day (i.e. extending holidays, floating holidays, etc.);\n",
    "- 'C' if the trip started on a day before a type-B day;\n",
    "- 'A' otherwise (i.e. a normal day, workday or weekend).\n",
    "\n",
    "IMPORTANT NOTICE: This field has not been correctly calculated. Please see the a reliable source for official holidays in Portugal.\n",
    "\n",
    "MISSING_DATA: (Boolean) It is FALSE when the GPS data stream is complete and TRUE whenever one (or more) locations are missing;\n",
    "\n",
    "POLYLINE: (String): It contains a list of GPS coordinates (i.e. WGS84 format) mapped as a string. The beginning and the end of the string are identified with brackets (i.e. [ and ], respectively). Each pair of coordinates is also identified by the same brackets as [LONGITUDE, LATITUDE]. This list contains one pair of coordinates for each 15 seconds of trip. The last list item corresponds to the trip's destination while the first one represents its start.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'data/Porto_taxi_data_training.csv'\n",
    "df = pd.read_csv(file, nrows=7732)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a simple trajectory plot on blank canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "from pyproj import Proj, transform # https://en.wikipedia.org/wiki/Web_Mercator_projection\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "inProj, outProj = Proj(init='epsg:4326'), Proj(init='epsg:3857')\n",
    "\n",
    "gps_x, gps_y = [], [] \n",
    "wmp_x, wmp_y = [], [] # web mercator projection\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "\n",
    "    trajectory = literal_eval(row[\"POLYLINE\"])\n",
    "    \n",
    "    x, y = transform(inProj, outProj, [xx[0] for xx in trajectory], [xx[1] for xx in trajectory])\n",
    "    \n",
    "    wmp_x.extend(x)\n",
    "    wmp_y.extend(y)\n",
    "    \n",
    "    gps_x.extend([xx[0] for xx in trajectory])\n",
    "    gps_y.extend([xx[1] for xx in trajectory])\n",
    "    \n",
    "    break    \n",
    "        \n",
    "p = figure()\n",
    "p.line(gps_x, gps_y, color = 'red', line_width=2)\n",
    "p.circle(gps_x, gps_y, fill_color=\"white\", size=8)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tile Provider Maps\n",
    "\n",
    "Bokeh plots can consume XYZ tile services which use the Web Mercator projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.models import BoxZoomTool\n",
    "from bokeh.plotting import figure, output_notebook, show\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "PORTO = x_range, y_range = ((-960000, -953500), (5015000, 5055000))\n",
    "\n",
    "plot_width  = int(750)\n",
    "plot_height = int(plot_width//1.2)\n",
    "\n",
    "def base_plot(tools='pan,wheel_zoom,reset',plot_width=plot_width, plot_height=plot_height, **plot_args):\n",
    "    p = figure(tools=tools, plot_width=plot_width, plot_height=plot_height,\n",
    "        x_range=x_range, y_range=y_range, outline_line_color=None,\n",
    "        min_border=0, min_border_left=0, min_border_right=0,\n",
    "        min_border_top=0, min_border_bottom=0, **plot_args)\n",
    "    \n",
    "    p.axis.visible = False\n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.ygrid.grid_line_color = None\n",
    "    \n",
    "    p.add_tools(BoxZoomTool(match_aspect=True))\n",
    "    \n",
    "    return p\n",
    "\n",
    "from bokeh.tile_providers import get_provider, CARTODBPOSITRON\n",
    "\n",
    "p = base_plot()\n",
    "p.add_tile(get_provider(CARTODBPOSITRON))\n",
    "p.line(wmp_x, wmp_y, color = 'red', line_width=2)\n",
    "p.circle(wmp_x, wmp_y, fill_color=\"white\", size=1)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get dates from timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "ts = int(\"1372636858\")\n",
    "\n",
    "print(datetime.utcfromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Google Maps (google map api credentials required, optional for this lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import output_file, show\n",
    "from bokeh.models import ColumnDataSource, GMapOptions\n",
    "from bokeh.plotting import gmap\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "map_options = GMapOptions(lat=-8.618643, lng=41.141412, map_type=\"roadmap\", zoom=11)\n",
    "\n",
    "p = gmap(\"Key\", map_options, title=\"PORTO\")\n",
    "\n",
    "p.circle(x=gps_x, y=gps_y, size=15, fill_color=\"blue\", fill_alpha=0.8)\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "### Task 5\n",
    "\n",
    "5a - How many Geo data samples do we have in these trips?\n",
    "\n",
    "5b - Which Area/district has the most taxi activities? Produce a plot to visualize it.\n",
    "\n",
    "5c - How busy are these street (in terms of taxi activities): i) during 6:00 - 10:00, ii) during 16:00 - 20:00 and iii) 22:00 - 06:00? Visualize it (making it interactive if necessary).\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
