{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8oXvWKo2JNwO"
   },
   "source": [
    "# Lab4: GPU Programming Lab\n",
    "\n",
    "### Name: Write your names (Group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8Vn0OiCJNwb"
   },
   "source": [
    "\n",
    "This lab is an introduction of GPU programming with cuda using python. The consists of 3 exercises and a homework. \n",
    "\n",
    "- <font color='red'><b> After each exercise, write a detailed summary explaining what you have done, your observations and  conclusions. </b></font>\n",
    "- <font color='red'><b> Make sure to write your name and your partner name (as registred in Halmstad University) in the name section above. </b></font>\n",
    "    \n",
    "- <font color='red'><b> You can do the lab in a group of a maximum of two students. </b></font>\n",
    "\n",
    "- <font color='red'><b> Only one of the students upload the lab to the blackboard. </b></font>\n",
    "\n",
    "# CUDA\n",
    "CUDA is a parallel programming platform and an API that facilitates the access to the CUDA-Enabled GPU functuonality for general purpose computing. It allows speeding up the software by utilizing the GPU power for the parallelizable part of the computation. Many Deep Learning platforms like tenserflow, keras, pytorch and others, rely on CUDA for their computations.\n",
    "\n",
    "## Common CUDA terminology:\n",
    "- <b>Host:</b> The CPU\n",
    "- <b>Device:</b> The GPU\n",
    "- <b>Host Memory:</b> The system main memory\n",
    "- <b>Device Memory:</b> The GPU onboard memory\n",
    "- <b>kernel:</b> A function that runs on the Device\n",
    "\n",
    "Threads are organized into a grid of blocks, where each block contains a subset of the threads that can cooperate using a block shared memory and can synchronize within each block.\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1QzXBVWki0M80KKY_CPzQu1ivE3fAcf2U' width=\"50%\" height=\"50%\"></img>\n",
    "\n",
    "\n",
    "Parallel portions of an application are executed on the device (GPU) as kernels, where an array of threads excutes each kernel. Each thread has an ID, by which it controls the portion of the data to excute the Kernel. All threads runs the same code on different portions of the data. Grids and Blocls can be organized as 1D, 2D, or 3D arrays. \n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1vqh749XFQhfZwq7m7E-VXscBblh58mei' width=\"50%\" height=\"50%\"></img>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bnWwv7hqJNwc"
   },
   "source": [
    "# Numba\n",
    "CUDA is designed to work with C++, but in this Lab we will work with Numba; a Python JIT compiler that translates subsets of the code into machine code, and enables writing a parallel GPU algorithms in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEQegGrzJNwe"
   },
   "source": [
    "# Kernel \n",
    "- A Kernel is declared as a function with @cuda.jit decorator.\n",
    "- A Kernel function cannot have a return value and manages outputs as input-output arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5X4C20WJNwd"
   },
   "source": [
    "## Numba installation\n",
    "\n",
    "\n",
    "conda install numba\n",
    "\n",
    "pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jw9u59gALbLS",
    "outputId": "e50a329a-47c6-4c05-e535-fe925808810b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "e8Q0qjGoJNwg"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import numba as nb\n",
    "from numba import cuda\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xLVEb941JNwi"
   },
   "outputs": [],
   "source": [
    "# kernel decleration\n",
    "@cuda.jit\n",
    "def my_kernel(io_array):\n",
    "    io_thread = cuda.grid(1)\n",
    "    io_array[io_thread] += 12\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COXIYxwCJNwj"
   },
   "source": [
    "To invoc a kernal you have to specify number of blocks in the grid, and the number of threads per block. This can be done by specifying the number of threads per block and calculating how many blocks are required in the grid based on the size of the data.\n",
    "\n",
    "<font color=red>Important note: In the case that the data size is not divisable by the the number of thread per block, we take the ceiling of the number to reserve an extra block for the remaining part of the data. So the threads in the last block will not be fully occupied.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ajW-s-FtJNwk"
   },
   "outputs": [],
   "source": [
    "# kernel invocation\n",
    "data = np.ones(256)\n",
    "\n",
    "threadsperblock = 32\n",
    "blockspergrid = math.ceil(len(data)/threadsperblock)\n",
    "\n",
    "my_kernel[blockspergrid, threadsperblock](data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2L6a_ADqJNwl"
   },
   "source": [
    "# Exercise 1: Element-wise operation\n",
    "\n",
    "The following kernel takes 1D array as input and computes the element-wise cube-root x^(1/3) for each element in the array. This an example of an arbitrary costy operation.\n",
    "\n",
    "- pos: holds the position in the data on which the thread will work.\n",
    "- Always check if the position exceeds the length of the data for the sake of cases when the data length is not devisable by the number of threads per block.\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1AndzjaLH-Lc7N4cg1Ue_zEB3EyJni89N' width=\"50%\" height=\"50%\"></img>\n",
    "\n",
    "Read the code below and compute the position of the thread on which it will do the computation in the output array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qESjmmQSJNwm",
    "outputId": "4b0ce6c4-a05c-4c00-8804-9387780934b5"
   },
   "outputs": [],
   "source": [
    "# kernel decleration\n",
    "@cuda.jit\n",
    "def my_kernel(io_array):\n",
    "    # Thread id in a 1D block\n",
    "    tx = cuda.threadIdx.x\n",
    "    # Block id in a 1D grid\n",
    "    bx = cuda.blockIdx.x\n",
    "    # Block width, i.e. number of threads per block\n",
    "    bw = cuda.blockDim.x\n",
    "    \n",
    "    # Compute flattened index inside the array\n",
    "    #pos = cuda.grid(1) # this function returns the same value for the position in a 1D grid\n",
    "    \n",
    "    #TODO: compute the correct pos value based on the tread index and the block index and the block width\n",
    "    pos = tx + (bx * bw)\n",
    "    \n",
    "    if pos < io_array.size:\n",
    "        io_array[pos] = io_array[pos]**(1/3)\n",
    "        \n",
    "\n",
    "# kernel invocation\n",
    "data = np.ones(2048)*27\n",
    "threadsperblock = 256\n",
    "blockspergrid = math.ceil(data.shape[0] / threadsperblock)\n",
    "my_kernel[blockspergrid, threadsperblock](data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IHcpGyXUJNwm",
    "outputId": "47c4a58e-0d34-4bcb-e948-08ea6786bd40"
   },
   "outputs": [],
   "source": [
    "data = np.ones(10000000)\n",
    "%timeit np.cbrt(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JdQ6m9lYJNwn",
    "outputId": "40c9192f-5703-492e-9396-9fd71cd1fd1f"
   },
   "outputs": [],
   "source": [
    "data = np.ones(10000000)\n",
    "threadsperblock = 1024\n",
    "blockspergrid = math.ceil(data.shape[0] / threadsperblock)\n",
    "%timeit my_kernel[blockspergrid, threadsperblock](data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndcV4wofJNwo"
   },
   "source": [
    "### Comparison between the previous kernel and Numpy \n",
    "- Try different array sizes and compare between CPU (using numpy) and GPU.\n",
    "- Plot a graph that shows the array sizes on the x axis and the computation time on the y axis of both your kernel and numpy (on the same plot). \n",
    "- Is there a relation between the size of the array and difference in performance? Explain what you notice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFgROdC7JNwo"
   },
   "source": [
    "### Exercise 1: Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 642
    },
    "id": "xOCdk-4sNoDB",
    "outputId": "6ec937a5-9c84-4512-bb0f-1ef484be5365"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Small Size \n",
    "start = time.time()\n",
    "data = np.ones(5000)\n",
    "%timeit np.cbrt(data)\n",
    "end = time.time()\n",
    "sm_cpu_time = end - start\n",
    "\n",
    "start = time.time()\n",
    "data = np.ones(50000000)\n",
    "%timeit np.cbrt(data)\n",
    "end = time.time()\n",
    "lg_cpu_time = end - start\n",
    "\n",
    "\n",
    "# Plotting for small size data\n",
    "\n",
    "plt.bar(['5000'], sm_cpu_time,  width=0.4)\n",
    "plt.bar(['50000000'], lg_cpu_time,  width=0.4)\n",
    "plt.xlabel(\"Array Size\")\n",
    "plt.ylabel(\"Running Time\")\n",
    "plt.title(\"CPU\")\n",
    "plt.show()\n",
    "\n",
    "# Large Size\n",
    "\n",
    "start = time.time()\n",
    "data = np.ones(5000)\n",
    "threadsperblock = 1024\n",
    "blockspergrid = math.ceil(data.shape[0] / threadsperblock)\n",
    "%timeit my_kernel[blockspergrid, threadsperblock](data)\n",
    "end = time.time()\n",
    "sm_gpu_time = end - start\n",
    "\n",
    "start = time.time()\n",
    "data = np.ones(50000000)\n",
    "threadsperblock = 1024\n",
    "blockspergrid = math.ceil(data.shape[0] / threadsperblock)\n",
    "%timeit my_kernel[blockspergrid, threadsperblock](data)\n",
    "end = time.time()\n",
    "lg_gpu_time = end - start\n",
    "\n",
    "# Plotting\n",
    "\n",
    "plt.bar(['5000'], sm_gpu_time,  width=0.4)\n",
    "plt.bar(['50000000'], lg_gpu_time,  width=0.4)\n",
    "plt.xlabel(\"Array Size\")\n",
    "plt.ylabel(\"Running Time\")\n",
    "plt.title(\"GPU\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# There is a relation between size and performance in both CPU and GPU. For smaller size CPU do better than GPU but as we increase the size the performance of GPU become far better as compare to CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFRUo1qOJNwp"
   },
   "source": [
    "## Exercise 2: Matrix Multiplication\n",
    "\n",
    "In matrix multiplication, every kernel will be reponsible of computing one element of the output matrix. It reads one row from the first matrix (A) and one column form the second matrix (B) and computes the dot product of these two vectors and place it in the corresponding cell in the output matrix (C) as shown in the following figure.\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=16EMuj46QLdwKmIDPU0P6AepZ9SNssb2s' width=\"50%\" height=\"50%\"></img>\n",
    "\n",
    "Write a kernel to do the multiplication of two matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZVUq5cSJNwp"
   },
   "outputs": [],
   "source": [
    "# CUDA kernel\n",
    "@cuda.jit\n",
    "def mat_mul(A, B, C):\n",
    "    \"\"\"Perform matrix multiplication of C = A * B\n",
    "    \"\"\"\n",
    "    # get the 2D position of the thread in which it will compute the dot product of the corresponding vectors \n",
    "    row, col = cuda.grid(2)\n",
    "    \n",
    "    if row < C.shape[0] and col < C.shape[1]:\n",
    "        #TODO: Compute the dot product \"prod\" of the corresponding vectors of this position \n",
    "        prod = 0.\n",
    "        for idx in range(A.shape[1]):\n",
    "            prod += A[row, idx] * B[idx, row]\n",
    "        C[row, col] = prod\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClcaDBMMJNwp"
   },
   "source": [
    "### Create a host function to invoke the kernel\n",
    "\n",
    "It is a good practice to manually copy the matrices to Device (the GPU memory) using \"cuda.to_device\" to reduce the unnecessary data transfer between the device and the host.\n",
    "\n",
    "\n",
    "To test the kernel \"mat_mul\" we prepare the host function \"gpu_dot\" which will take two matrices as parameters and returns the the output matrix. The job of this host function is to perpare the data and to invoke the kernel.\n",
    "\n",
    "Read the code below and calculate how many blocks are required to start the kernel. Use the calculated values to invoke the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "id": "cim3RIExJNwq",
    "outputId": "b9d36eb3-8844-4cfc-f2fc-95a3da1fa0dc"
   },
   "outputs": [],
   "source": [
    "def gpu_dot(A, B):\n",
    "    #Copy the input matrices to the gpu\n",
    "    start_copy_time = time.time()\n",
    "    A_global_mem = cuda.to_device(A)\n",
    "    B_global_mem = cuda.to_device(B)\n",
    "\n",
    "    # Allocate memory on the device for the result (Note the shape of the output matrix)\n",
    "    C_global_mem = cuda.device_array((A.shape[0], B.shape[1]), np.float32)\n",
    "    \n",
    "    # Configure the blocks\n",
    "    # Specify how many threads per block\n",
    "    threadsperblock = (32, 32)\n",
    "    \n",
    "    #TODO: Calculate how many blocks are required\n",
    "    blockspergrid_x = int(math.ceil(A.shape[0] / threadsperblock[0]))\n",
    "    blockspergrid_y = int(math.ceil(B.shape[1] / threadsperblock[1]))\n",
    "    blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "    dt = time.time()-start_copy_time\n",
    "    print(f'Copying Input to GPU time: {dt} s')\n",
    "    start_mult_time = time.time()\n",
    "    \n",
    "    #TODO: Start the kernel based on the calculated grid \n",
    "    mat_mul[blockspergrid, threadsperblock](A_global_mem, B_global_mem, C_global_mem)\n",
    "    \n",
    "    dt = time.time()-start_mult_time\n",
    "    print(f'Multiplication Time: {dt} s')\n",
    "    # Copy the result back to the host\n",
    "    start_copy_back_time = time.time()\n",
    "    C = C_global_mem.copy_to_host()\n",
    "    dt = time.time()-start_copy_back_time\n",
    "    print(f'Copy result back time: {dt} s')\n",
    "    dt = time.time()-start_copy_time\n",
    "    print(f'Total time: {dt} s')\n",
    "    return C\n",
    "\n",
    "# Input Test arrays\n",
    "A = np.full((16384, 2048), 3, np.float32) # matrix containing all 3's\n",
    "B = np.full((2048, 16384), 4, np.float32) # matrix containing all 4's\n",
    "\n",
    "#Test the host function\n",
    "C = gpu_dot(A,B)\n",
    "print(f'Input Shapes:A:{A.shape}, B:{B.shape}')\n",
    "\n",
    "print('Output Shape:', C.shape)\n",
    "print('Output:',C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48DAlsYdJNwr"
   },
   "source": [
    "### Testing the calculations time compared to numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nEjQvFDqJNwr",
    "outputId": "c43bc4c3-b744-4cdd-ce1b-8a08115fe99c"
   },
   "outputs": [],
   "source": [
    "%timeit np.dot(A,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJMqM-swJNwr"
   },
   "source": [
    "### Comparison between the previous gpu_dot and Numpy.dot\n",
    "- Try different array sizes and compare between CPU (using np.dot) and GPU (using gpu_dot).\n",
    "- Plot a graph that shows the array sizes (bacause it is a 2D matrix, you can consider the size to be the hight x width) on the x axis and the computation time on the y axis of both your kernel and numpy (on the same plot). \n",
    "- Explain what you notice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FTyrwdSJNws"
   },
   "source": [
    "### Exercise 2: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 658
    },
    "id": "81KHy0dh9Dvz",
    "outputId": "3c9fd173-24ad-4c45-c5fe-25c8ad46ee3d"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Small Size \n",
    "start = time.time()\n",
    "\n",
    "A = np.full((2500, 500), 3, np.float32) # matrix containing all 3's\n",
    "B = np.full((500, 2500), 4, np.float32) # matrix containing all 4's\n",
    "\n",
    "np.dot(A,B)\n",
    "\n",
    "end = time.time()\n",
    "sm_cpu_time = end - start\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "A = np.full((2500, 500), 3, np.float32) # matrix containing all 3's\n",
    "B = np.full((500, 2500), 4, np.float32) # matrix containing all 4's\n",
    "\n",
    "np.dot(A,B)\n",
    "\n",
    "end = time.time()\n",
    "lg_cpu_time = end - start\n",
    "\n",
    "\n",
    "# Plotting for small size data\n",
    "\n",
    "plt.bar(['2500 x 500'], sm_cpu_time,  width=0.4)\n",
    "plt.bar(['25000 x 5000'], lg_cpu_time,  width=0.4)\n",
    "plt.xlabel(\"Size\")\n",
    "plt.ylabel(\"Running Time\")\n",
    "plt.title(\"CPU\")\n",
    "plt.show()\n",
    "\n",
    "# GPU \n",
    "# Small Size\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "A = np.full((2500, 500), 3, np.float32) # matrix containing all 3's\n",
    "B = np.full((500, 2500), 4, np.float32) # matrix containing all 4's\n",
    "\n",
    "gpu_dot(A,B)\n",
    "\n",
    "end = time.time()\n",
    "sm_gpu_time = end - start\n",
    "# Small Size\n",
    "start = time.time()\n",
    "A = np.full((25000, 5000), 3, np.float32) # matrix containing all 3's\n",
    "B = np.full((5000, 25000), 4, np.float32) # matrix containing all 4's\n",
    "\n",
    "gpu_dot(A,B)\n",
    "\n",
    "end = time.time()\n",
    "lg_gpu_time = end - start\n",
    "\n",
    "# Plotting\n",
    "\n",
    "plt.bar(['2500 x 500'], sm_gpu_time,  width=0.4)\n",
    "plt.bar(['25000 x 5000'], lg_gpu_time,  width=0.4)\n",
    "plt.xlabel(\"Size\")\n",
    "plt.ylabel(\"Running Time\")\n",
    "plt.title(\"GPU\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# The gpu_dot performs better than cpu_dot as data's matrices copy to device and calculations done ther before tranfering to host"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M6fhf9lMJNws"
   },
   "source": [
    "## Exercise 3: Distance Matrix\n",
    "The distance matrix (D) of a data matrix (A) is the matrix that contains the eucleadian distance between each two row vectors as shown in the following figure.\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1UMMRYmtPW9_Tonq20GBjxsDLrNFYSTdc' width=\"50%\" height=\"50%\"></img>\n",
    "\n",
    "where \n",
    "$$D[i,j]=D[j,i]=dist(A[i,:], A[j,:])$$\n",
    "\n",
    "\n",
    "Use what you have learned in the previous exercises to write a kernel and a host function to compute the distance matrix of a data matrix. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "NWoMKJj-JNws",
    "outputId": "cc4e9520-0871-4876-b394-a2911c38edcf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "@cuda.jit()\n",
    "def distance_matrix(mat, out):\n",
    "    #TODO: write a kernel to compute the distance matrix of the input \"mat\" and place the result in \"out\"\n",
    "    row, column = cuda.grid(2)\n",
    "\n",
    "    distance = 0\n",
    "    if row < mat.shape[0] and column < mat.shape[1]:\n",
    "        for i in range(mat.shape[1]):\n",
    "            distance +=(mat[row, i] - mat[column, i])**2\n",
    "\n",
    "        out[row, column] = math.sqrt(distance)\n",
    "\n",
    "def gpu_dist_matrix(mat):\n",
    "    #TODO: write a host function to calculate the grid size and use the calculated values to invoke the \"distance_Matrix\" kernel\n",
    "    row = mat.shape[0]\n",
    "    column = mat.shape[1]\n",
    "\n",
    "    blocks = 32\n",
    "    grid_dim = (int(row/blocks), int(column/blocks))\n",
    "  \n",
    "    stream = cuda.stream()\n",
    "    inMatrix = cuda.to_device(np.asarray(mat)) \n",
    "    outMatrix = cuda.device_array((row, column)) \n",
    "    distance_matrix[grid_dim, (blocks, blocks)](inMatrix, outMatrix) \n",
    "    outMatrix = outMatrix.copy_to_host(stream=stream)\n",
    "   \n",
    "    return outMatrix\n",
    "\n",
    "\n",
    "A = np.random.randn(1024,1024)\n",
    "D = gpu_dist_matrix(A)\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKdNMS4tJNwt"
   },
   "source": [
    "# Homework: K-Nearest Neighbors (GPU version)\n",
    "\n",
    "K-Nearest Neighbors is one of the simplest and most intuitive algorithms in machine learning that relies on the principle that close points behave similarly. It is one of the case-based learning algorithms that can learn non-linear complicated decision boundaries with a single hyperparameter, i.e. K the number of nearest neighbors. The problem of this algorithm is that, to find the k nearest neighbors of a specific point, you have to compute the distances to all the points in the training dataset, which is very costly in terms of computation especially with a large amount of data. A great benefit can be achieved by performing such computation on the GPU.\n",
    "\n",
    "Your task is to implement the K-Nearest Neighbors algorithm using python, and Numba, and CUDA programming.\n",
    "Identify the parts of the algorithm that can make use of the GPU and implement them as CUDA kernels.\n",
    "\n",
    "Use the MNIST dataset as an example and implement a K-Nearest Neighbors classifier to classify the image of the digit into its category.\n",
    "\n",
    "Try different numbers of K and figure out the number that maximizes the accuracy of the classifier.\n",
    "Build another K-Nearest Neighbors using the Sciket-learn library and compare the computation time with your GPU-enabled algorithm. \n",
    "\n",
    "You can download MNIST from Keras library: ( https://keras.io/api/datasets/mnist/ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAUocKI-JNwu"
   },
   "source": [
    "### Homework: Reported Time and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1dTnQE2DJNwu",
    "outputId": "05f66d65-25ac-48d5-9494-23b6760551c1"
   },
   "outputs": [],
   "source": [
    "# GPU-enabled KNNNeighbour\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "\n",
    "(X_train, y_train),(X_test, y_test) = tf.keras.datasets.mnist.load_data(path='mnist.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pztOTkFNF77F"
   },
   "outputs": [],
   "source": [
    "# When k=1\n",
    "output = []\n",
    "for i in range(len(X_test)):\n",
    "    distances = []\n",
    "    points = []\n",
    "    for j in range(len(X_train)):\n",
    "        row = np.array(X_train[j],X_test[i])\n",
    "        column = j\n",
    "\n",
    "        blocks = 32\n",
    "        grid_dim = (int(row/blocks), int(column/blocks))\n",
    "\n",
    "        stream = cuda.stream()\n",
    "        inMatrix = cuda.to_device(np.asarray(np.array(X_train[j],X_test[i]))) \n",
    "        outMatrix = cuda.device_array((row, column)) \n",
    "        distance_matrix[grid_dim, (blocks, blocks)](inMatrix, outMatrix) \n",
    "        outMatrix = outMatrix.copy_to_host(stream=stream)\n",
    "        distances.append(outMatrix)\n",
    "      \n",
    "    distances.sort()\n",
    "    distances = distances[0:1]\n",
    "    for d, j in distances:\n",
    "        points.append(y_train[j])\n",
    "        ans = Counter(points).most_common(1)[0][0]\n",
    "        output.append(ans)\n",
    "\n",
    "accuracy = (output == y_test).sum() / len(y_test)\n",
    "print(f\"K = 1 : {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lkHg0iAnZ9xr"
   },
   "outputs": [],
   "source": [
    "# When k=5\n",
    "output = []\n",
    "for i in range(len(X_test)):\n",
    "    distances = []\n",
    "    points = []\n",
    "    for j in range(len(X_train)):\n",
    "        row = np.array(X_train[j],X_test[i])\n",
    "        column = j\n",
    "\n",
    "        blocks = 32\n",
    "        grid_dim = (int(row/blocks), int(column/blocks))\n",
    "\n",
    "        stream = cuda.stream()\n",
    "        inMatrix = cuda.to_device(np.asarray(np.array(X_train[j],X_test[i]))) \n",
    "        outMatrix = cuda.device_array((row, column)) \n",
    "        distance_matrix[grid_dim, (blocks, blocks)](inMatrix, outMatrix) \n",
    "        outMatrix = outMatrix.copy_to_host(stream=stream)\n",
    "        distances.append(outMatrix)\n",
    "      \n",
    "    distances.sort()\n",
    "    distances = distances[0:5]\n",
    "    for d, j in distances:\n",
    "        points.append(y_train[j])\n",
    "        ans = Counter(points).most_common(1)[0][0]\n",
    "        output.append(ans)\n",
    "\n",
    "accuracy = (output == y_test).sum() / len(y_test)\n",
    "print(f\"K = 5 : {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When k=25\n",
    "output = []\n",
    "for i in range(len(X_test)):\n",
    "    distances = []\n",
    "    points = []\n",
    "    for j in range(len(X_train)):\n",
    "         row = np.array(X_train[j],X_test[i])\n",
    "        column = j\n",
    "\n",
    "        blocks = 32\n",
    "        grid_dim = (int(row/blocks), int(column/blocks))\n",
    "\n",
    "        stream = cuda.stream()\n",
    "        inMatrix = cuda.to_device(np.asarray(np.array(X_train[j],X_test[i]))) \n",
    "        outMatrix = cuda.device_array((row, column)) \n",
    "        distance_matrix[grid_dim, (blocks, blocks)](inMatrix, outMatrix) \n",
    "        outMatrix = outMatrix.copy_to_host(stream=stream)\n",
    "        distances.append(outMatrix)\n",
    "      \n",
    "    distances.sort()\n",
    "    distances = distances[0:25]\n",
    "    for d, j in distances:\n",
    "        points.append(y_train[j])\n",
    "        ans = Counter(points).most_common(1)[0][0]\n",
    "        output.append(ans)\n",
    "\n",
    "accuracy = (output == y_test).sum() / len(y_test)\n",
    "print(f\"K = 25 : {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3bDhqj2MbDO_",
    "outputId": "6d9ca92b-c269-434b-c43a-a38a20e7cdd7"
   },
   "outputs": [],
   "source": [
    "# KNN using Sklearn\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "mnist_dataset = datasets.load_digits()\n",
    "\n",
    "(X_train, X_test, y_train, y_test) = train_test_split(np.array(mnist_dataset.data), mnist_dataset.target, test_size=0.25, random_state=42)\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors= 1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "test_score = clf.score(X_test, y_test)\n",
    "\n",
    "print(f\"K = 1 : {test_score}\")\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors= 5)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "test_score = clf.score(X_test, y_test)\n",
    "\n",
    "print(f\"K = 5 : {test_score}\")\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors= 25)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "test_score = clf.score(X_test, y_test)\n",
    "\n",
    "print(f\"K = 25 : {test_score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kk-YV33EkB_W"
   },
   "outputs": [],
   "source": [
    "# The lower the value of K, the more it'll be accurate. \n",
    "# The Computation time of GPU enabled one is lower than the Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ej-naGmInLy7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab4_StudentVersion(1).ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
