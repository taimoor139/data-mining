{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bvHXlsQdWt1N"
   },
   "source": [
    "# Lab4: GPU Programming Lab\n",
    "\n",
    "### Name: Write your names (Group)\n",
    "\n",
    "#1) Hafiz Muhammad Abubaker siddique\n",
    "##siddiquemuhammadabubaker915@gmail.com\n",
    "##hafsid21@students.hh.se\n",
    "##199312257794\n",
    "\n",
    "#2) Rana Abdul Basit Javaid\n",
    "##abdulbasitjavaid@gmail.com\n",
    "##199102230431"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PDu8uS3zWt1b"
   },
   "source": [
    "\n",
    "This lab is an introduction of GPU programming with cuda using python. The consists of 3 exercises and a homework. \n",
    "\n",
    "- <font color='red'><b> After each exercise, write a detailed summary explaining what you have done, your observations and  conclusions. </b></font>\n",
    "- <font color='red'><b> Make sure to write your name and your partner name (as registred in Halmstad University) in the name section above. </b></font>\n",
    "    \n",
    "- <font color='red'><b> You can do the lab in a group of a maximum of two students. </b></font>\n",
    "\n",
    "- <font color='red'><b> Only one of the students upload the lab to the blackboard. </b></font>\n",
    "\n",
    "# CUDA\n",
    "CUDA is a parallel programming platform and an API that facilitates the access to the CUDA-Enabled GPU functuonality for general purpose computing. It allows speeding up the software by utilizing the GPU power for the parallelizable part of the computation. Many Deep Learning platforms like tenserflow, keras, pytorch and others, rely on CUDA for their computations.\n",
    "\n",
    "## Common CUDA terminology:\n",
    "- <b>Host:</b> The CPU\n",
    "- <b>Device:</b> The GPU\n",
    "- <b>Host Memory:</b> The system main memory\n",
    "- <b>Device Memory:</b> The GPU onboard memory\n",
    "- <b>kernel:</b> A function that runs on the Device\n",
    "\n",
    "Threads are organized into a grid of blocks, where each block contains a subset of the threads that can cooperate using a block shared memory and can synchronize within each block.\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1QzXBVWki0M80KKY_CPzQu1ivE3fAcf2U' width=\"50%\" height=\"50%\"></img>\n",
    "\n",
    "\n",
    "Parallel portions of an application are executed on the device (GPU) as kernels, where an array of threads excutes each kernel. Each thread has an ID, by which it controls the portion of the data to excute the Kernel. All threads runs the same code on different portions of the data. Grids and Blocls can be organized as 1D, 2D, or 3D arrays. \n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1vqh749XFQhfZwq7m7E-VXscBblh58mei' width=\"50%\" height=\"50%\"></img>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6r9loZkNWt1j"
   },
   "source": [
    "# Numba\n",
    "CUDA is designed to work with C++, but in this Lab we will work with Numba; a Python JIT compiler that translates subsets of the code into machine code, and enables writing a parallel GPU algorithms in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOtqtXg9Wt1l"
   },
   "source": [
    "## Numba installation\n",
    "\n",
    "\n",
    "conda install numba\n",
    "\n",
    "pip install numba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6aHhlB69Wt1z"
   },
   "source": [
    "# Kernel \n",
    "- A Kernel is declared as a function with @cuda.jit decorator.\n",
    "- A Kernel function cannot have a return value and manages outputs as input-output arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Ir3c_cCWWt11"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import numba as nb\n",
    "from numba import cuda\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "WMj0sjYTWt13"
   },
   "outputs": [],
   "source": [
    "# kernel decleration\n",
    "@cuda.jit\n",
    "def my_kernel(io_array):\n",
    "    threadId = cuda.grid(1)\n",
    "    io_array[threadId] += 10\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Yabic61Wt15"
   },
   "source": [
    "To invoc a kernal you have to specify number of blocks in the grid, and the number of threads per block. This can be done by specifying the number of threads per block and calculating how many blocks are required in the grid based on the size of the data.\n",
    "\n",
    "<font color=red>Important note: In the case that the data size is not divisable by the the number of thread per block, we take the ceiling of the number to reserve an extra block for the remaining part of the data. So the threads in the last block will not be fully occupied.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "H4fO18UMWt16"
   },
   "outputs": [
    {
     "ename": "CudaDriverError",
     "evalue": "driver missing function: cuDeviceGetUuid.\nRequires CUDA 9.2 or above.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCudaDriverError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m threadsperblock \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[0;32m      5\u001b[0m blockspergrid \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mceil(\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;241m/\u001b[39mthreadsperblock)\n\u001b[1;32m----> 7\u001b[0m \u001b[43mmy_kernel\u001b[49m\u001b[43m[\u001b[49m\u001b[43mblockspergrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreadsperblock\u001b[49m\u001b[43m]\u001b[49m(data)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numba\\cuda\\compiler.py:862\u001b[0m, in \u001b[0;36mDispatcher.__getitem__\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m]:\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmust specify at least the griddim and blockdim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 862\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfigure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numba\\cuda\\compiler.py:857\u001b[0m, in \u001b[0;36mDispatcher.configure\u001b[1;34m(self, griddim, blockdim, stream, sharedmem)\u001b[0m\n\u001b[0;32m    855\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconfigure\u001b[39m(\u001b[38;5;28mself\u001b[39m, griddim, blockdim, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, sharedmem\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    856\u001b[0m     griddim, blockdim \u001b[38;5;241m=\u001b[39m normalize_kernel_dimensions(griddim, blockdim)\n\u001b[1;32m--> 857\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_KernelConfiguration\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgriddim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblockdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msharedmem\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numba\\cuda\\compiler.py:718\u001b[0m, in \u001b[0;36m_KernelConfiguration.__init__\u001b[1;34m(self, dispatcher, griddim, blockdim, stream, sharedmem)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msharedmem \u001b[38;5;241m=\u001b[39m sharedmem\n\u001b[0;32m    717\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mCUDA_LOW_OCCUPANCY_WARNINGS:\n\u001b[1;32m--> 718\u001b[0m     ctx \u001b[38;5;241m=\u001b[39m \u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    719\u001b[0m     smcount \u001b[38;5;241m=\u001b[39m ctx\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mMULTIPROCESSOR_COUNT\n\u001b[0;32m    720\u001b[0m     grid_size \u001b[38;5;241m=\u001b[39m griddim[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m griddim[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m griddim[\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numba\\cuda\\cudadrv\\devices.py:220\u001b[0m, in \u001b[0;36mget_context\u001b[1;34m(devnum)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_context\u001b[39m(devnum\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;124;03m\"\"\"Get the current device or use a device by device number, and\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;124;03m    return the CUDA context.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_runtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_or_create_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevnum\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numba\\cuda\\cudadrv\\devices.py:138\u001b[0m, in \u001b[0;36m_Runtime.get_or_create_context\u001b[1;34m(self, devnum)\u001b[0m\n\u001b[0;32m    136\u001b[0m attached_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_attached_context()\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attached_ctx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_or_create_context_uncached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevnum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attached_ctx\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numba\\cuda\\cudadrv\\devices.py:155\u001b[0m, in \u001b[0;36m_Runtime._get_or_create_context_uncached\u001b[1;34m(self, devnum)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m driver\u001b[38;5;241m.\u001b[39mget_active_context() \u001b[38;5;28;01mas\u001b[39;00m ac:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ac:\n\u001b[1;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_activate_context_for\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;66;03m# Get primary context for the active device\u001b[39;00m\n\u001b[0;32m    158\u001b[0m         ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpus[ac\u001b[38;5;241m.\u001b[39mdevnum]\u001b[38;5;241m.\u001b[39mget_primary_context()\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numba\\cuda\\cudadrv\\devices.py:176\u001b[0m, in \u001b[0;36m_Runtime._activate_context_for\u001b[1;34m(self, devnum)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_activate_context_for\u001b[39m(\u001b[38;5;28mself\u001b[39m, devnum):\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m--> 176\u001b[0m         gpu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpus\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdevnum\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    177\u001b[0m         newctx \u001b[38;5;241m=\u001b[39m gpu\u001b[38;5;241m.\u001b[39mget_primary_context()\n\u001b[0;32m    178\u001b[0m         \u001b[38;5;66;03m# Detect unexpected context switch\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numba\\cuda\\cudadrv\\devices.py:40\u001b[0m, in \u001b[0;36m_DeviceList.__getitem__\u001b[1;34m(self, devnum)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, devnum):\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    Returns the context manager for device *devnum*.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlst\u001b[49m[devnum]\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numba\\cuda\\cudadrv\\devices.py:27\u001b[0m, in \u001b[0;36m_DeviceList.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlst\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# Device list is not initialized.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Query all CUDA devices.\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     numdev \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mget_device_count()\n\u001b[1;32m---> 27\u001b[0m     gpus \u001b[38;5;241m=\u001b[39m [_DeviceContextManager(driver\u001b[38;5;241m.\u001b[39mget_device(devid))\n\u001b[0;32m     28\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m devid \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(numdev)]\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# Define \"lst\" to avoid re-initialization\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlst \u001b[38;5;241m=\u001b[39m gpus\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numba\\cuda\\cudadrv\\devices.py:27\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlst\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# Device list is not initialized.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Query all CUDA devices.\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     numdev \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mget_device_count()\n\u001b[1;32m---> 27\u001b[0m     gpus \u001b[38;5;241m=\u001b[39m [_DeviceContextManager(\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevid\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     28\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m devid \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(numdev)]\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# Define \"lst\" to avoid re-initialization\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlst \u001b[38;5;241m=\u001b[39m gpus\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numba\\cuda\\cudadrv\\driver.py:407\u001b[0m, in \u001b[0;36mDriver.get_device\u001b[1;34m(self, devnum)\u001b[0m\n\u001b[0;32m    405\u001b[0m dev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevices\u001b[38;5;241m.\u001b[39mget(devnum)\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dev \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 407\u001b[0m     dev \u001b[38;5;241m=\u001b[39m \u001b[43mDevice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevnum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    408\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevices[devnum] \u001b[38;5;241m=\u001b[39m dev\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m weakref\u001b[38;5;241m.\u001b[39mproxy(dev)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numba\\cuda\\cudadrv\\driver.py:596\u001b[0m, in \u001b[0;36mDevice.__init__\u001b[1;34m(self, devnum)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    595\u001b[0m     uuid \u001b[38;5;241m=\u001b[39m cu_uuid()\n\u001b[1;32m--> 596\u001b[0m     \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuDeviceGetUuid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43muuid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    597\u001b[0m     uuid_vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mbytes\u001b[39m(uuid))\n\u001b[0;32m    599\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%02x\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numba\\cuda\\cudadrv\\driver.py:318\u001b[0m, in \u001b[0;36mDriver._ctypes_wrap_fn.<locals>.safe_cuda_api_call\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msafe_cuda_api_call\u001b[39m(\u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m    317\u001b[0m     _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall driver api: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m, libfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m--> 318\u001b[0m     retcode \u001b[38;5;241m=\u001b[39m \u001b[43mlibfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_ctypes_error(fname, retcode)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numba\\cuda\\cudadrv\\driver.py:369\u001b[0m, in \u001b[0;36mDriver._find_api.<locals>.absent_function\u001b[1;34m(*args, **kws)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mabsent_function\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkws):\n\u001b[1;32m--> 369\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CudaDriverError(MISSING_FUNCTION_ERRMSG \u001b[38;5;241m%\u001b[39m fname)\n",
      "\u001b[1;31mCudaDriverError\u001b[0m: driver missing function: cuDeviceGetUuid.\nRequires CUDA 9.2 or above.\n"
     ]
    }
   ],
   "source": [
    "# kernel invocation\n",
    "data = np.ones(256)\n",
    "\n",
    "threadsperblock = 32\n",
    "blockspergrid = math.ceil(len(data)/threadsperblock)\n",
    "\n",
    "my_kernel[blockspergrid, threadsperblock](data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tfo6xB3EWt17"
   },
   "source": [
    "# Exercise 1: Element-wise operation\n",
    "\n",
    "The following kernel takes 1D array as input and computes the element-wise cube-root x^(1/3) for each element in the array. This an example of an arbitrary costy operation.\n",
    "\n",
    "- pos: holds the position in the data on which the thread will work.\n",
    "- Always check if the position exceeds the length of the data for the sake of cases when the data length is not devisable by the number of threads per block.\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1AndzjaLH-Lc7N4cg1Ue_zEB3EyJni89N' width=\"50%\" height=\"50%\"></img>\n",
    "\n",
    "Read the code below and compute the position of the thread on which it will do the computation in the output array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i2Dd4PPYWt18",
    "outputId": "a9a8a301-a716-4779-8554-cabba7256125"
   },
   "outputs": [],
   "source": [
    "# kernel decleration\n",
    "@cuda.jit\n",
    "def my_kernel(io_array):\n",
    "    # Thread id in a 1D block\n",
    "    tx = cuda.threadIdx.x\n",
    "    # Block id in a 1D grid\n",
    "    ty = cuda.blockIdx.x\n",
    "     # Block width, i.e. number of threads per block\n",
    "    bw = cuda.blockDim.x\n",
    "    \n",
    "    \n",
    "    # Compute flattened index inside the array\n",
    "    #pos = cuda.grid(1) # this function returns the same value for the position in a 1D grid\n",
    "    \n",
    "    #TODO: compute the correct pos value based on the tread index and the block index and the block width\n",
    "    pos = tx + ty * bw\n",
    "\n",
    "    \n",
    "    if pos < io_array.size:\n",
    "        io_array[pos] = io_array[pos]**(1/3)\n",
    "        \n",
    "\n",
    "# kernel invocation\n",
    "data = np.ones(2048)*27\n",
    "threadsperblock = 256\n",
    "blockspergrid = math.ceil(data.shape[0] / threadsperblock)\n",
    "my_kernel[blockspergrid, threadsperblock](data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "icC4alkzWt19",
    "outputId": "fdfce834-f806-4958-9530-ce928679fdc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "890 ms ± 15.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "data = np.ones(10000000)\n",
    "%timeit np.cbrt(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cmJvlTJiWt1-",
    "outputId": "82b008c8-5b92-4bad-c342-504b16520194"
   },
   "outputs": [
    {
     "ename": "CudaDriverError",
     "evalue": "driver missing function: cuDeviceGetUuid.\nRequires CUDA 9.2 or above.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCudaDriverError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m threadsperblock \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024\u001b[39m\n\u001b[0;32m      3\u001b[0m blockspergrid \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mceil(data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m threadsperblock)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtimeit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmy_kernel[blockspergrid, threadsperblock](data)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2294\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[1;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[0;32m   2292\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[0;32m   2293\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m-> 2294\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\magics\\execution.py:1162\u001b[0m, in \u001b[0;36mExecutionMagics.timeit\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1160\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m   1161\u001b[0m     number \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m index\n\u001b[1;32m-> 1162\u001b[0m     time_number \u001b[38;5;241m=\u001b[39m \u001b[43mtimer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumber\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m time_number \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m:\n\u001b[0;32m   1164\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\magics\\execution.py:156\u001b[0m, in \u001b[0;36mTimer.timeit\u001b[1;34m(self, number)\u001b[0m\n\u001b[0;32m    154\u001b[0m gc\u001b[38;5;241m.\u001b[39mdisable()\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 156\u001b[0m     timing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gcold:\n",
      "File \u001b[1;32m<magic-timeit>:1\u001b[0m, in \u001b[0;36minner\u001b[1;34m(_it, _timer)\u001b[0m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numba\\cuda\\compiler.py:862\u001b[0m, in \u001b[0;36mDispatcher.__getitem__\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m]:\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmust specify at least the griddim and blockdim\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 862\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfigure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numba\\cuda\\compiler.py:857\u001b[0m, in \u001b[0;36mDispatcher.configure\u001b[1;34m(self, griddim, blockdim, stream, sharedmem)\u001b[0m\n\u001b[0;32m    855\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconfigure\u001b[39m(\u001b[38;5;28mself\u001b[39m, griddim, blockdim, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, sharedmem\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    856\u001b[0m     griddim, blockdim \u001b[38;5;241m=\u001b[39m normalize_kernel_dimensions(griddim, blockdim)\n\u001b[1;32m--> 857\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_KernelConfiguration\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgriddim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblockdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msharedmem\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numba\\cuda\\compiler.py:718\u001b[0m, in \u001b[0;36m_KernelConfiguration.__init__\u001b[1;34m(self, dispatcher, griddim, blockdim, stream, sharedmem)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msharedmem \u001b[38;5;241m=\u001b[39m sharedmem\n\u001b[0;32m    717\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mCUDA_LOW_OCCUPANCY_WARNINGS:\n\u001b[1;32m--> 718\u001b[0m     ctx \u001b[38;5;241m=\u001b[39m \u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    719\u001b[0m     smcount \u001b[38;5;241m=\u001b[39m ctx\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mMULTIPROCESSOR_COUNT\n\u001b[0;32m    720\u001b[0m     grid_size \u001b[38;5;241m=\u001b[39m griddim[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m griddim[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m griddim[\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numba\\cuda\\cudadrv\\devices.py:220\u001b[0m, in \u001b[0;36mget_context\u001b[1;34m(devnum)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_context\u001b[39m(devnum\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;124;03m\"\"\"Get the current device or use a device by device number, and\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;124;03m    return the CUDA context.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_runtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_or_create_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevnum\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numba\\cuda\\cudadrv\\devices.py:138\u001b[0m, in \u001b[0;36m_Runtime.get_or_create_context\u001b[1;34m(self, devnum)\u001b[0m\n\u001b[0;32m    136\u001b[0m attached_ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_attached_context()\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attached_ctx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_or_create_context_uncached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevnum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attached_ctx\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numba\\cuda\\cudadrv\\devices.py:155\u001b[0m, in \u001b[0;36m_Runtime._get_or_create_context_uncached\u001b[1;34m(self, devnum)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m driver\u001b[38;5;241m.\u001b[39mget_active_context() \u001b[38;5;28;01mas\u001b[39;00m ac:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ac:\n\u001b[1;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_activate_context_for\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;66;03m# Get primary context for the active device\u001b[39;00m\n\u001b[0;32m    158\u001b[0m         ctx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpus[ac\u001b[38;5;241m.\u001b[39mdevnum]\u001b[38;5;241m.\u001b[39mget_primary_context()\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numba\\cuda\\cudadrv\\devices.py:176\u001b[0m, in \u001b[0;36m_Runtime._activate_context_for\u001b[1;34m(self, devnum)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_activate_context_for\u001b[39m(\u001b[38;5;28mself\u001b[39m, devnum):\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m--> 176\u001b[0m         gpu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpus\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdevnum\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    177\u001b[0m         newctx \u001b[38;5;241m=\u001b[39m gpu\u001b[38;5;241m.\u001b[39mget_primary_context()\n\u001b[0;32m    178\u001b[0m         \u001b[38;5;66;03m# Detect unexpected context switch\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numba\\cuda\\cudadrv\\devices.py:40\u001b[0m, in \u001b[0;36m_DeviceList.__getitem__\u001b[1;34m(self, devnum)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, devnum):\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    Returns the context manager for device *devnum*.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlst\u001b[49m[devnum]\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numba\\cuda\\cudadrv\\devices.py:27\u001b[0m, in \u001b[0;36m_DeviceList.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlst\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# Device list is not initialized.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Query all CUDA devices.\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     numdev \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mget_device_count()\n\u001b[1;32m---> 27\u001b[0m     gpus \u001b[38;5;241m=\u001b[39m [_DeviceContextManager(driver\u001b[38;5;241m.\u001b[39mget_device(devid))\n\u001b[0;32m     28\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m devid \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(numdev)]\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# Define \"lst\" to avoid re-initialization\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlst \u001b[38;5;241m=\u001b[39m gpus\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numba\\cuda\\cudadrv\\devices.py:27\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlst\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# Device list is not initialized.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Query all CUDA devices.\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     numdev \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mget_device_count()\n\u001b[1;32m---> 27\u001b[0m     gpus \u001b[38;5;241m=\u001b[39m [_DeviceContextManager(\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevid\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     28\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m devid \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(numdev)]\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# Define \"lst\" to avoid re-initialization\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlst \u001b[38;5;241m=\u001b[39m gpus\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numba\\cuda\\cudadrv\\driver.py:407\u001b[0m, in \u001b[0;36mDriver.get_device\u001b[1;34m(self, devnum)\u001b[0m\n\u001b[0;32m    405\u001b[0m dev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevices\u001b[38;5;241m.\u001b[39mget(devnum)\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dev \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 407\u001b[0m     dev \u001b[38;5;241m=\u001b[39m \u001b[43mDevice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevnum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    408\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevices[devnum] \u001b[38;5;241m=\u001b[39m dev\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m weakref\u001b[38;5;241m.\u001b[39mproxy(dev)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numba\\cuda\\cudadrv\\driver.py:596\u001b[0m, in \u001b[0;36mDevice.__init__\u001b[1;34m(self, devnum)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    595\u001b[0m     uuid \u001b[38;5;241m=\u001b[39m cu_uuid()\n\u001b[1;32m--> 596\u001b[0m     \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuDeviceGetUuid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43muuid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    597\u001b[0m     uuid_vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mbytes\u001b[39m(uuid))\n\u001b[0;32m    599\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%02x\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numba\\cuda\\cudadrv\\driver.py:318\u001b[0m, in \u001b[0;36mDriver._ctypes_wrap_fn.<locals>.safe_cuda_api_call\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msafe_cuda_api_call\u001b[39m(\u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m    317\u001b[0m     _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall driver api: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m, libfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m--> 318\u001b[0m     retcode \u001b[38;5;241m=\u001b[39m \u001b[43mlibfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_ctypes_error(fname, retcode)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numba\\cuda\\cudadrv\\driver.py:369\u001b[0m, in \u001b[0;36mDriver._find_api.<locals>.absent_function\u001b[1;34m(*args, **kws)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mabsent_function\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkws):\n\u001b[1;32m--> 369\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CudaDriverError(MISSING_FUNCTION_ERRMSG \u001b[38;5;241m%\u001b[39m fname)\n",
      "\u001b[1;31mCudaDriverError\u001b[0m: driver missing function: cuDeviceGetUuid.\nRequires CUDA 9.2 or above.\n"
     ]
    }
   ],
   "source": [
    "data = np.ones(10000000)\n",
    "threadsperblock = 1024\n",
    "blockspergrid = math.ceil(data.shape[0] / threadsperblock)\n",
    "%timeit my_kernel[blockspergrid, threadsperblock](data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFanifasWt1-"
   },
   "source": [
    "### Comparison between the previous kernel and Numpy \n",
    "- Try different array sizes and compare between CPU (using numpy) and GPU.\n",
    "- Plot a graph that shows the array sizes on the x axis and the computation time on the y axis of both your kernel and numpy (on the same plot). \n",
    "- Is there a relation between the size of the array and difference in performance? Explain what you notice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7DyA-RVmWt1_"
   },
   "source": [
    "### Exercise 1: Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t5Q1f7veWt1_"
   },
   "source": [
    "## Exercise 2: Matrix Multiplication\n",
    "\n",
    "In matrix multiplication, every kernel will be reponsible of computing one element of the output matrix. It reads one row from the first matrix (A) and one column form the second matrix (B) and computes the dot product of these two vectors and place it in the corresponding cell in the output matrix (C) as shown in the following figure.\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=16EMuj46QLdwKmIDPU0P6AepZ9SNssb2s' width=\"50%\" height=\"50%\"></img>\n",
    "\n",
    "Write a kernel to do the multiplication of two matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "6ZHEGku145ra",
    "outputId": "58a73d8e-e31c-4e1d-b004-b4a54f43fcd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 loops, best of 5: 197 µs per loop\n",
      "The slowest run took 4.28 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "1000 loops, best of 5: 600 µs per loop\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Small Size CPU vs GPU\n",
    "\n",
    "smallData = np.ones(10000)\n",
    "\n",
    "# CPU\n",
    "startTime = time.time()\n",
    "%timeit np.cbrt(smallData)\n",
    "stopTime = time.time()\n",
    "totalCPUTime = stopTime - startTime\n",
    "\n",
    "# GPU\n",
    "startTime = time.time()\n",
    "threadsperblock = 1024\n",
    "blockspergrid = math.ceil(smallData[0] / threadsperblock)\n",
    "%timeit my_kernel[blockspergrid, threadsperblock](smallData)\n",
    "stopTime = time.time()\n",
    "totalGPUTime = stopTime - startTime\n",
    "\n",
    "# Plot\n",
    "\n",
    "plt.bar([totalCPUTime], smallData, width=0.05)\n",
    "plt.bar([totalGPUTime], smallData, width=0.05)\n",
    "plt.xlabel(\"Data Size\")\n",
    "plt.ylabel(\"Time\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sGA16AN49py2",
    "outputId": "0ae433b0-9078-4c1e-c78a-8f3c3c0b33eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 5: 197 ms per loop\n",
      "10 loops, best of 5: 35.7 ms per loop\n"
     ]
    }
   ],
   "source": [
    "# Large Size CPU vs GPU\n",
    "\n",
    "largeData = np.ones(10000000)\n",
    "\n",
    "# CPU\n",
    "startTime = time.time()\n",
    "%timeit np.cbrt(largeData)\n",
    "stopTime = time.time()\n",
    "totalCPUTime = stopTime - startTime\n",
    "\n",
    "# GPU\n",
    "startTime = time.time()\n",
    "threadsperblock = 1024\n",
    "blockspergrid = math.ceil(largeData[0] / threadsperblock)\n",
    "%timeit my_kernel[blockspergrid, threadsperblock](largeData)\n",
    "stopTime = time.time()\n",
    "totalGPUTime = stopTime - startTime\n",
    "\n",
    "# Plot\n",
    "\n",
    "plt.bar([totalCPUTime], largeData, width=0.05)\n",
    "plt.bar([totalGPUTime], largeData, width=0.05)\n",
    "plt.xlabel(\"Data Size\")\n",
    "plt.ylabel(\"Time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-hO435iOWt1_"
   },
   "outputs": [],
   "source": [
    "# CUDA kernel\n",
    "@cuda.jit\n",
    "def matmul(A, B, C):\n",
    "    \"\"\"Perform matrix multiplication of C = A * B\n",
    "    \"\"\"\n",
    "    row, col = cuda.grid(2)\n",
    "    if row < C.shape[0] and col < C.shape[1]:\n",
    "        tmp = 0.\n",
    "        for k in range(A.shape[1]):\n",
    "            tmp += A[row, k] * B[k, col]\n",
    "        C[row, col] = tmp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCj5fXbjWt2A"
   },
   "source": [
    "### Create a host function to invoke the kernel\n",
    "\n",
    "It is a good practice to manually copy the matrices to Device (the GPU memory) using \"cuda.to_device\" to reduce the unnecessary data transfer between the device and the host.\n",
    "\n",
    "\n",
    "To test the kernel \"mat_mul\" we prepare the host function \"gpu_dot\" which will take two matrices as parameters and returns the the output matrix. The job of this host function is to perpare the data and to invoke the kernel.\n",
    "\n",
    "Read the code below and calculate how many blocks are required to start the kernel. Use the calculated values to invoke the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z72fjiM6Wt2A",
    "outputId": "fa8854e9-03e5-4dc1-805d-75f97246e70d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying Input to GPU time: 0.05836033821105957 s\n",
      "Multiplication Time: 0.16926980018615723 s\n",
      "Copy result back time: 0.460146427154541 s\n",
      "Total time: 0.6881165504455566 s\n",
      "Input Shapes:A:(16384, 2048), B:(2048, 16384)\n"
     ]
    }
   ],
   "source": [
    "def gpu_dot(A, B):\n",
    "    #Copy the input matrices to the gpu\n",
    "    start_copy_time = time.time()\n",
    "    A_global_mem = cuda.to_device(A)\n",
    "    B_global_mem = cuda.to_device(B)\n",
    "\n",
    "    # Allocate memory on the device for the result (Note the shape of the output matrix)\n",
    "    C_global_mem = cuda.device_array((A.shape[0], B.shape[1]), np.float32)\n",
    "    \n",
    "    # Configure the blocks\n",
    "    # Specify how many threads per block\n",
    "    threadsperblock = (32, 32)\n",
    "    \n",
    "    #TODO: Calculate how many blocks are required\n",
    "    Xblockspergrid =  int(math.ceil(A.shape[0] / threadsperblock[0]))\n",
    "    Yblockspergrid =  int(math.ceil(B.shape[1] / threadsperblock[1]))\n",
    "    blockspergrid = (Xblockspergrid, Yblockspergrid)\n",
    "\n",
    "    dt = time.time()-start_copy_time\n",
    "    print(f'Copying Input to GPU time: {dt} s')\n",
    "    start_mult_time = time.time()\n",
    "    \n",
    "    #TODO: Start the kernel based on the calculated grid \n",
    "    matmul[blockspergrid, threadsperblock](A_global_mem, B_global_mem, C_global_mem)\n",
    "    \n",
    "    dt = time.time()-start_mult_time\n",
    "    print(f'Multiplication Time: {dt} s')\n",
    "    # Copy the result back to the host\n",
    "    start_copy_back_time = time.time()\n",
    "    C = C_global_mem.copy_to_host()\n",
    "    dt = time.time()-start_copy_back_time\n",
    "    print(f'Copy result back time: {dt} s')\n",
    "    dt = time.time()-start_copy_time\n",
    "    print(f'Total time: {dt} s')\n",
    "    return C\n",
    "\n",
    "# Input Test arrays\n",
    "A = np.full((16384, 2048), 3, np.float32) # matrix containing all 3's\n",
    "B = np.full((2048, 16384), 4, np.float32) # matrix containing all 4's\n",
    "\n",
    "#Test the host function\n",
    "C = gpu_dot(A,B)\n",
    "print(f'Input Shapes:A:{A.shape}, B:{B.shape}')\n",
    "\n",
    "print('Output Shape:', C.shape)\n",
    "print('Output:',C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ag6XVZTDWt2B"
   },
   "source": [
    "### Testing the calculations time compared to numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zL2L5bwOWt2B",
    "outputId": "66ce44a8-1ea7-4ebb-e01d-6f184a40ca9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 5: 12.7 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit np.dot(A,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifa5Ku81Wt2C"
   },
   "source": [
    "### Comparison between the previous gpu_dot and Numpy.dot\n",
    "- Try different array sizes and compare between CPU (using np.dot) and GPU (using gpu_dot).\n",
    "- Plot a graph that shows the array sizes (bacause it is a 2D matrix, you can consider the size to be the hight x width) on the x axis and the computation time on the y axis of both your kernel and numpy (on the same plot). \n",
    "- Explain what you notice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_6218i1Wt2C"
   },
   "source": [
    "### Exercise 2: Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f_txBrn7AFsB"
   },
   "outputs": [],
   "source": [
    "# Small Size CPU vs GPU\n",
    "\n",
    "X = np.full((1000, 100), 3, np.float32)\n",
    "Y = np.full((100, 1000), 4, np.float32)\n",
    "\n",
    "# CPU\n",
    "startTime = time.time()\n",
    "%timeit np.dot(X, Y)\n",
    "stopTime = time.time()\n",
    "totalCPUTime = stopTime - startTime\n",
    "\n",
    "# GPU\n",
    "startTime = time.time()\n",
    "%timeit gpu_dot(X, Y)\n",
    "stopTime = time.time()\n",
    "totalGPUTime = stopTime - startTime\n",
    "\n",
    "# Plot\n",
    "\n",
    "plt.bar([totalCPUTime], '1000 x 100', width=0.05)\n",
    "plt.bar([totalGPUTime], '1000 x 100', width=0.05)\n",
    "plt.xlabel(\"Data\")\n",
    "plt.ylabel(\"Time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HAkcP0XvEQTf"
   },
   "outputs": [],
   "source": [
    "# Large Size CPU vs GPU\n",
    "\n",
    "X = np.full((100000, 100), 3, np.float32)\n",
    "Y = np.full((100, 100000), 4, np.float32)\n",
    "\n",
    "# CPU\n",
    "startTime = time.time()\n",
    "%timeit np.dot(X, Y)\n",
    "stopTime = time.time()\n",
    "totalCPUTime = stopTime - startTime\n",
    "\n",
    "# GPU\n",
    "startTime = time.time()\n",
    "%timeit gpu_dot(X, Y)\n",
    "stopTime = time.time()\n",
    "totalGPUTime = stopTime - startTime\n",
    "\n",
    "# Plot\n",
    "\n",
    "plt.bar([totalCPUTime], '1000 x 100', width=0.05)\n",
    "plt.bar([totalGPUTime], '1000 x 100', width=0.05)\n",
    "plt.xlabel(\"Data\")\n",
    "plt.ylabel(\"Time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CM6JFyCGWt2D"
   },
   "source": [
    "## Exercise 3: Distance Matrix\n",
    "The distance matrix (D) of a data matrix (A) is the matrix that contains the eucleadian distance between each two row vectors as shown in the following figure.\n",
    "\n",
    "<img src='https://drive.google.com/uc?id=1UMMRYmtPW9_Tonq20GBjxsDLrNFYSTdc' width=\"50%\" height=\"50%\"></img>\n",
    "\n",
    "where \n",
    "$$D[i,j]=D[j,i]=dist(A[i,:], A[j,:])$$\n",
    "\n",
    "\n",
    "Use what you have learned in the previous exercises to write a kernel and a host function to compute the distance matrix of a data matrix. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GKdw3V0hWt2D",
    "outputId": "e0869a4d-c546-4d72-e688-ef2d0bb1b5d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0.         2232.76206222 2049.08279642 ... 1928.84672522\n",
      "  2012.31993927 2202.44881006]\n",
      " [2232.76206222    0.         2044.95132355 ... 1956.39805137\n",
      "  1937.33295833 2147.55623843]\n",
      " [2049.08279642 2044.95132355    0.         ... 2079.8856032\n",
      "  1917.3095798  2048.77443963]\n",
      " ...\n",
      " [1928.84672522 1956.39805137 2079.8856032  ...    0.\n",
      "  1983.91926463 1992.91436212]\n",
      " [2012.31993927 1937.33295833 1917.3095798  ... 1983.91926463\n",
      "     0.         1904.72800995]\n",
      " [2202.44881006 2147.55623843 2048.77443963 ... 1992.91436212\n",
      "  1904.72800995    0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numba import cuda\n",
    "\n",
    "USE_64 = True\n",
    "\n",
    "if USE_64:\n",
    "    bits = 64\n",
    "    np_type = np.float64\n",
    "else:\n",
    "    bits = 32\n",
    "    np_type = np.float32\n",
    "\n",
    "@cuda.jit(\"void(float{}[:, :], float{}[:, :])\".format(bits, bits))\n",
    "def distance_matrix(mat, out):\n",
    "    #TODO: write a kernel to compute the distance matrix of the input \"mat\" and place the result in \"out\"\n",
    "    row, column = cuda.grid(2)\n",
    "\n",
    "    distance = 0\n",
    "    if row < inMatrix.shape[0] and column < inMatrix.shape[1]:\n",
    "        for i in range(inMatrix.shape[1]):\n",
    "            distance +=(inMatrix[row, i] - inMatrix[column, i])**2\n",
    "\n",
    "        outMatrix[row, column] = math.sqrt(distance)\n",
    "\n",
    "def gpu_dist_matrix(mat):\n",
    "    #TODO: write a host function to calculate the grid size and use the calculated values to invoke the \"distance_Matrix\" kernel\n",
    "    row = matrix.shape[0]\n",
    "    column = matrix.shape[1]\n",
    "\n",
    "    blocks = 32\n",
    "    grid_dim = (int(row/blocks), int(column/blocks))\n",
    "  \n",
    "    stream = cuda.stream()\n",
    "    inMatrix = cuda.to_device(np.asarray(matrix)) \n",
    "    outMatrix = cuda.device_array((row, column)) \n",
    "    distance_matrix[grid_dim, (blocks, blocks)](inMatrix, outMatrix)\n",
    "    outMatrix = outMatrix.copy_to_host(stream=stream)\n",
    "   \n",
    "    return outMatrix\n",
    "\n",
    "\n",
    "A = np.random.randn(1024,1024)\n",
    "D = gpu_dist_matrix(A)\n",
    "print(D)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XcG2xhBwWt2E"
   },
   "source": [
    "# Homework: K-Nearest Neighbors (GPU version)\n",
    "\n",
    "K-Nearest Neighbors is one of the simplest and most intuitive algorithms in machine learning that relies on the principle that close points behave similarly. It is one of the case-based learning algorithms that can learn non-linear complicated decision boundaries with a single hyperparameter, i.e. K the number of nearest neighbors. The problem of this algorithm is that, to find the k nearest neighbors of a specific point, you have to compute the distances to all the points in the training dataset, which is very costly in terms of computation especially with a large amount of data. A great benefit can be achieved by performing such computation on the GPU.\n",
    "\n",
    "Your task is to implement the K-Nearest Neighbors algorithm using python, and Numba, and CUDA programming.\n",
    "Identify the parts of the algorithm that can make use of the GPU and implement them as CUDA kernels.\n",
    "\n",
    "Use the MNIST dataset as an example and implement a K-Nearest Neighbors classifier to classify the image of the digit into its category.\n",
    "\n",
    "Try different numbers of K and figure out the number that maximizes the accuracy of the classifier.\n",
    "Build another K-Nearest Neighbors using the Sciket-learn library and compare the computation time with your GPU-enabled algorithm. \n",
    "\n",
    "You can download MNIST from Keras library: ( https://keras.io/api/datasets/mnist/ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1hklsRpWt2F"
   },
   "source": [
    "### Homework: Reported Time and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gMNrBX51Wt2F"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import datasets\n",
    "from skimage import exposure\n",
    "import numpy as np\n",
    "import imutils\n",
    "import cv2\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5VPE1Se2Wt2F",
    "outputId": "f9dfb145-642c-4408-bb9e-6d9092fb321e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imutils in /usr/local/lib/python3.7/dist-packages (0.5.4)\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9NoJ2E2uWt2G",
    "outputId": "93e41edf-92e1-44ef-dee5-a7dae6e07fda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (4.1.2.30)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python) (1.21.6)\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WP098Oo-Wt2G"
   },
   "outputs": [],
   "source": [
    "#if int((sklearn.__version__).split(\".\")[1]) < 18:\n",
    "\t#from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ygMvTc3gWt2G"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load the MNIST digits dataset\n",
    "mnist = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NXlvinEdWt2H"
   },
   "outputs": [],
   "source": [
    "# take the MNIST data and construct the training and testing split, using 75% of the\n",
    "# data for training and 25% for testing\n",
    "(trainData, testData, trainLabels, testLabels) = train_test_split(np.array(mnist.data),\n",
    "\tmnist.target, test_size=0.25, random_state=42)\n",
    "\n",
    "# now, let's take 10% of the training data and use that for validation\n",
    "(trainData, valData, trainLabels, valLabels) = train_test_split(trainData, trainLabels,\n",
    "\ttest_size=0.1, random_state=84)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yUQ3UQBVWt2H",
    "outputId": "c7a1b035-08e2-4052-82aa-49054698126e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data points: 1212\n",
      "validation data points: 135\n",
      "testing data points: 450\n"
     ]
    }
   ],
   "source": [
    "# show the sizes of each data split\n",
    "print(\"training data points: {}\".format(len(trainLabels)))\n",
    "print(\"validation data points: {}\".format(len(valLabels)))\n",
    "print(\"testing data points: {}\".format(len(testLabels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DqQUED5aWt2H"
   },
   "outputs": [],
   "source": [
    "# initialize the values of k for our k-Nearest Neighbor classifier along with the\n",
    "# list of accuracies for each value of k\n",
    "kVals = range(1, 30, 2)\n",
    "accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QMTpaQlpWt2I",
    "outputId": "ab2e9dd7-1fe8-4df3-fe95-24c3138f5412"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=1, accuracy=99.26%\n",
      "k=3, accuracy=99.26%\n",
      "k=5, accuracy=99.26%\n",
      "k=7, accuracy=99.26%\n",
      "k=9, accuracy=99.26%\n",
      "k=11, accuracy=99.26%\n",
      "k=13, accuracy=99.26%\n",
      "k=15, accuracy=99.26%\n",
      "k=17, accuracy=98.52%\n",
      "k=19, accuracy=98.52%\n",
      "k=21, accuracy=97.78%\n",
      "k=23, accuracy=97.04%\n",
      "k=25, accuracy=97.78%\n",
      "k=27, accuracy=97.04%\n",
      "k=29, accuracy=97.04%\n",
      "k=1 achieved highest accuracy of 99.26% on validation data\n"
     ]
    }
   ],
   "source": [
    "# loop over various values of `k` for the k-Nearest Neighbor classifier\n",
    "for k in range(1, 30, 2):\n",
    "\t# train the k-Nearest Neighbor classifier with the current value of `k`\n",
    "\tmodel = KNeighborsClassifier(n_neighbors=k)\n",
    "\tmodel.fit(trainData, trainLabels)\n",
    "\n",
    "\t# evaluate the model and update the accuracies list\n",
    "\tscore = model.score(valData, valLabels)\n",
    "\tprint(\"k=%d, accuracy=%.2f%%\" % (k, score * 100))\n",
    "\taccuracies.append(score)\n",
    "\n",
    "# find the value of k that has the largest accuracy\n",
    "i = int(np.argmax(accuracies))\n",
    "print(\"k=%d achieved highest accuracy of %.2f%% on validation data\" % (kVals[i],\n",
    "\taccuracies[i] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T0IrYtIpWt2I",
    "outputId": "abc90ef6-b6f5-4f51-ff3d-c906b582c911"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION ON TESTING DATA\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        43\n",
      "           1       0.95      1.00      0.97        37\n",
      "           2       1.00      1.00      1.00        38\n",
      "           3       0.98      0.98      0.98        46\n",
      "           4       0.98      0.98      0.98        55\n",
      "           5       0.98      1.00      0.99        59\n",
      "           6       1.00      1.00      1.00        45\n",
      "           7       1.00      0.98      0.99        41\n",
      "           8       0.97      0.95      0.96        38\n",
      "           9       0.96      0.94      0.95        48\n",
      "\n",
      "    accuracy                           0.98       450\n",
      "   macro avg       0.98      0.98      0.98       450\n",
      "weighted avg       0.98      0.98      0.98       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# re-train our classifier using the best k value and predict the labels of the\n",
    "# test data\n",
    "model = KNeighborsClassifier(n_neighbors=kVals[i])\n",
    "model.fit(trainData, trainLabels)\n",
    "predictions = model.predict(testData)\n",
    "\n",
    "# show a final classification report demonstrating the accuracy of the classifier\n",
    "# for each of the digits\n",
    "print(\"EVALUATION ON TESTING DATA\")\n",
    "print(classification_report(testLabels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F9oX6MNWWt2J",
    "outputId": "6c69f996-ee67-4522-cf9e-8932bff95fe1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I think that digit is: 4\n",
      "I think that digit is: 8\n",
      "I think that digit is: 6\n",
      "I think that digit is: 3\n",
      "I think that digit is: 2\n"
     ]
    }
   ],
   "source": [
    "# loop over a few random digits\n",
    "for i in list(map(int, np.random.randint(0, high=len(testLabels), size=(5,)))):\n",
    "\t# grab the image and classify it\n",
    "\timage = testData[i]\n",
    "\tprediction = model.predict(image.reshape(1, -1))[0]\n",
    "\n",
    "\t# convert the image for a 64-dim array to an 8 x 8 image compatible with OpenCV,\n",
    "\t# then resize it to 32 x 32 pixels so we can see it better\n",
    "\timage = image.reshape((8, 8)).astype(\"uint8\")\n",
    "\timage = exposure.rescale_intensity(image, out_range=(0, 255))\n",
    "\timage = imutils.resize(image, width=32, inter=cv2.INTER_CUBIC)\n",
    "\n",
    "\t# show the prediction\n",
    "\tprint(\"I think that digit is: {}\".format(prediction))\n",
    "\t#cv2.imshow(\"Image\", image)\n",
    "\tcv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "juQWs6M6Wt2M"
   },
   "outputs": [],
   "source": [
    "# KNN using GPU\n",
    "\n",
    "# KNN where k=5\n",
    "\n",
    "knnOutput = []\n",
    "for i in range(len(trainLabel)):\n",
    "    distances = []\n",
    "    points = []\n",
    "    for j in range(len(trainData)):\n",
    "        row = np.array(trainData[j],trainLabel[i])\n",
    "        column = j\n",
    "\n",
    "        blocks = 32\n",
    "        grid_dim = (int(row/blocks), int(column/blocks))\n",
    "\n",
    "        stream = cuda.stream()\n",
    "        inMatrix = cuda.to_device(np.asarray(matrix)) \n",
    "        outMatrix = cuda.device_array((row, column)) \n",
    "        distance_matrix[grid_dim, (blocks, blocks)](inMatrix, outMatrix)\n",
    "        outMatrix = outMatrix.copy_to_host(stream=stream)\n",
    "        distances.append(outMatrix)\n",
    "      \n",
    "    distances.sort()\n",
    "    distances = distances[0:5]\n",
    "    for d, j in distances:\n",
    "        points.append(testData[j])\n",
    "        knnOutput.append(Counter(points).most_common(1)[0][0])\n",
    "\n",
    "result = (knnOutput == testLabel).sum() / len(testLabel) \n",
    "print(\"Accuracy when K=5\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sq_vcWvFTA-5"
   },
   "outputs": [],
   "source": [
    "# KNN where k=15\n",
    "\n",
    "knnOutput = []\n",
    "for i in range(len(trainLabel)):\n",
    "    distances = []\n",
    "    points = []\n",
    "    for j in range(len(trainData)):\n",
    "        row = np.array(trainData[j],trainLabel[i])\n",
    "        column = j\n",
    "\n",
    "        blocks = 32\n",
    "        grid_dim = (int(row/blocks), int(column/blocks))\n",
    "\n",
    "        stream = cuda.stream()\n",
    "        inMatrix = cuda.to_device(np.asarray(matrix)) \n",
    "        outMatrix = cuda.device_array((row, column)) \n",
    "        distance_matrix[grid_dim, (blocks, blocks)](inMatrix, outMatrix)\n",
    "        outMatrix = outMatrix.copy_to_host(stream=stream)\n",
    "        distances.append(outMatrix)\n",
    "      \n",
    "      \n",
    "    distances.sort()\n",
    "    distances = distances[0:15]\n",
    "    for d, j in distances:\n",
    "        points.append(testData[j])\n",
    "        knnOutput.append(Counter(points).most_common(1)[0][0])\n",
    "\n",
    "result = (knnOutput == testLabel).sum() / len(testLabel) \n",
    "print(\"Accuracy when K=15\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iz-7FkeqTBqs"
   },
   "outputs": [],
   "source": [
    "# KNN where k=30\n",
    "\n",
    "knnOutput = []\n",
    "for i in range(len(trainLabel)):\n",
    "    distances = []\n",
    "    points = []\n",
    "    for j in range(len(trainData)):\n",
    "        row = np.array(trainData[j],trainLabel[i])\n",
    "        column = j\n",
    "\n",
    "        blocks = 32\n",
    "        grid_dim = (int(row/blocks), int(column/blocks))\n",
    "\n",
    "        stream = cuda.stream()\n",
    "        inMatrix = cuda.to_device(np.asarray(matrix)) \n",
    "        outMatrix = cuda.device_array((row, column)) \n",
    "        distance_matrix[grid_dim, (blocks, blocks)](inMatrix, outMatrix)\n",
    "        outMatrix = outMatrix.copy_to_host(stream=stream)\n",
    "        distances.append(outMatrix)\n",
    "      \n",
    "      \n",
    "    distances.sort()\n",
    "    distances = distances[0:30]\n",
    "    for d, j in distances:\n",
    "        points.append(testData[j])\n",
    "        knnOutput.append(Counter(points).most_common(1)[0][0])\n",
    "\n",
    "result = (knnOutput == testLabel).sum() / len(testLabel) \n",
    "print(\"Accuracy when K=30\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "ifa5Ku81Wt2C"
   ],
   "name": "Lab_4_Abubaker_siddique_&_Abdul_basit (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
