{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oXvWKo2JNwO"
      },
      "source": [
        "# Lab4: GPU Programming Lab\n",
        "\n",
        "### Name: Write your names (Group)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8Vn0OiCJNwb"
      },
      "source": [
        "\n",
        "This lab is an introduction of GPU programming with cuda using python. The consists of 3 exercises and a homework. \n",
        "\n",
        "- <font color='red'><b> After each exercise, write a detailed summary explaining what you have done, your observations and  conclusions. </b></font>\n",
        "- <font color='red'><b> Make sure to write your name and your partner name (as registred in Halmstad University) in the name section above. </b></font>\n",
        "    \n",
        "- <font color='red'><b> You can do the lab in a group of a maximum of two students. </b></font>\n",
        "\n",
        "- <font color='red'><b> Only one of the students upload the lab to the blackboard. </b></font>\n",
        "\n",
        "# CUDA\n",
        "CUDA is a parallel programming platform and an API that facilitates the access to the CUDA-Enabled GPU functuonality for general purpose computing. It allows speeding up the software by utilizing the GPU power for the parallelizable part of the computation. Many Deep Learning platforms like tenserflow, keras, pytorch and others, rely on CUDA for their computations.\n",
        "\n",
        "## Common CUDA terminology:\n",
        "- <b>Host:</b> The CPU\n",
        "- <b>Device:</b> The GPU\n",
        "- <b>Host Memory:</b> The system main memory\n",
        "- <b>Device Memory:</b> The GPU onboard memory\n",
        "- <b>kernel:</b> A function that runs on the Device\n",
        "\n",
        "Threads are organized into a grid of blocks, where each block contains a subset of the threads that can cooperate using a block shared memory and can synchronize within each block.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1QzXBVWki0M80KKY_CPzQu1ivE3fAcf2U' width=\"50%\" height=\"50%\"></img>\n",
        "\n",
        "\n",
        "Parallel portions of an application are executed on the device (GPU) as kernels, where an array of threads excutes each kernel. Each thread has an ID, by which it controls the portion of the data to excute the Kernel. All threads runs the same code on different portions of the data. Grids and Blocls can be organized as 1D, 2D, or 3D arrays. \n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1vqh749XFQhfZwq7m7E-VXscBblh58mei' width=\"50%\" height=\"50%\"></img>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnWwv7hqJNwc"
      },
      "source": [
        "# Numba\n",
        "CUDA is designed to work with C++, but in this Lab we will work with Numba; a Python JIT compiler that translates subsets of the code into machine code, and enables writing a parallel GPU algorithms in Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEQegGrzJNwe"
      },
      "source": [
        "# Kernel \n",
        "- A Kernel is declared as a function with @cuda.jit decorator.\n",
        "- A Kernel function cannot have a return value and manages outputs as input-output arrays"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5X4C20WJNwd"
      },
      "source": [
        "## Numba installation\n",
        "\n",
        "\n",
        "conda install numba\n",
        "\n",
        "pip install numba"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numba"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jw9u59gALbLS",
        "outputId": "e50a329a-47c6-4c05-e535-fe925808810b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (0.51.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from numba) (1.21.6)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba) (0.34.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "e8Q0qjGoJNwg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import numba as nb\n",
        "from numba import cuda\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xLVEb941JNwi"
      },
      "outputs": [],
      "source": [
        "# kernel decleration\n",
        "@cuda.jit\n",
        "def my_kernel(io_array):\n",
        "    io_thread = cuda.grid(1)\n",
        "    io_array[io_thread] += 12\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COXIYxwCJNwj"
      },
      "source": [
        "To invoc a kernal you have to specify number of blocks in the grid, and the number of threads per block. This can be done by specifying the number of threads per block and calculating how many blocks are required in the grid based on the size of the data.\n",
        "\n",
        "<font color=red>Important note: In the case that the data size is not divisable by the the number of thread per block, we take the ceiling of the number to reserve an extra block for the remaining part of the data. So the threads in the last block will not be fully occupied.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ajW-s-FtJNwk"
      },
      "outputs": [],
      "source": [
        "# kernel invocation\n",
        "data = np.ones(256)\n",
        "\n",
        "threadsperblock = 32\n",
        "blockspergrid = math.ceil(len(data)/threadsperblock)\n",
        "\n",
        "my_kernel[blockspergrid, threadsperblock](data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2L6a_ADqJNwl"
      },
      "source": [
        "# Exercise 1: Element-wise operation\n",
        "\n",
        "The following kernel takes 1D array as input and computes the element-wise cube-root x^(1/3) for each element in the array. This an example of an arbitrary costy operation.\n",
        "\n",
        "- pos: holds the position in the data on which the thread will work.\n",
        "- Always check if the position exceeds the length of the data for the sake of cases when the data length is not devisable by the number of threads per block.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1AndzjaLH-Lc7N4cg1Ue_zEB3EyJni89N' width=\"50%\" height=\"50%\"></img>\n",
        "\n",
        "Read the code below and compute the position of the thread on which it will do the computation in the output array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qESjmmQSJNwm",
        "outputId": "4b0ce6c4-a05c-4c00-8804-9387780934b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3. 3. 3. ... 3. 3. 3.]\n"
          ]
        }
      ],
      "source": [
        "# kernel decleration\n",
        "@cuda.jit\n",
        "def my_kernel(io_array):\n",
        "    # Thread id in a 1D block\n",
        "    tx = cuda.threadIdx.x\n",
        "    # Block id in a 1D grid\n",
        "    bx = cuda.blockIdx.x\n",
        "    # Block width, i.e. number of threads per block\n",
        "    bw = cuda.blockDim.x\n",
        "    \n",
        "    # Compute flattened index inside the array\n",
        "    #pos = cuda.grid(1) # this function returns the same value for the position in a 1D grid\n",
        "    \n",
        "    #TODO: compute the correct pos value based on the tread index and the block index and the block width\n",
        "    pos = tx + (bx * bw)\n",
        "    \n",
        "    if pos < io_array.size:\n",
        "        io_array[pos] = io_array[pos]**(1/3)\n",
        "        \n",
        "\n",
        "# kernel invocation\n",
        "data = np.ones(2048)*27\n",
        "threadsperblock = 256\n",
        "blockspergrid = math.ceil(data.shape[0] / threadsperblock)\n",
        "my_kernel[blockspergrid, threadsperblock](data)\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHcpGyXUJNwm",
        "outputId": "47c4a58e-0d34-4bcb-e948-08ea6786bd40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 218 ms per loop\n"
          ]
        }
      ],
      "source": [
        "data = np.ones(10000000)\n",
        "%timeit np.cbrt(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdQ6m9lYJNwn",
        "outputId": "40c9192f-5703-492e-9396-9fd71cd1fd1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 loops, best of 5: 42.9 ms per loop\n"
          ]
        }
      ],
      "source": [
        "data = np.ones(10000000)\n",
        "threadsperblock = 1024\n",
        "blockspergrid = math.ceil(data.shape[0] / threadsperblock)\n",
        "%timeit my_kernel[blockspergrid, threadsperblock](data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndcV4wofJNwo"
      },
      "source": [
        "### Comparison between the previous kernel and Numpy \n",
        "- Try different array sizes and compare between CPU (using numpy) and GPU.\n",
        "- Plot a graph that shows the array sizes on the x axis and the computation time on the y axis of both your kernel and numpy (on the same plot). \n",
        "- Is there a relation between the size of the array and difference in performance? Explain what you notice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFgROdC7JNwo"
      },
      "source": [
        "### Exercise 1: Results\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Small Size \n",
        "start = time.time()\n",
        "data = np.ones(5000)\n",
        "%timeit np.cbrt(data)\n",
        "end = time.time()\n",
        "sm_cpu_time = end - start\n",
        "\n",
        "start = time.time()\n",
        "data = np.ones(50000000)\n",
        "%timeit np.cbrt(data)\n",
        "end = time.time()\n",
        "lg_cpu_time = end - start\n",
        "\n",
        "\n",
        "# Plotting for small size data\n",
        "\n",
        "plt.bar(['5000'], sm_cpu_time,  width=0.4)\n",
        "plt.bar(['50000000'], lg_cpu_time,  width=0.4)\n",
        "plt.xlabel(\"Array Size\")\n",
        "plt.ylabel(\"Running Time\")\n",
        "plt.title(\"CPU\")\n",
        "plt.show()\n",
        "\n",
        "# Large Size\n",
        "\n",
        "start = time.time()\n",
        "data = np.ones(5000)\n",
        "threadsperblock = 1024\n",
        "blockspergrid = math.ceil(data.shape[0] / threadsperblock)\n",
        "%timeit my_kernel[blockspergrid, threadsperblock](data)\n",
        "end = time.time()\n",
        "sm_gpu_time = end - start\n",
        "\n",
        "start = time.time()\n",
        "data = np.ones(50000000)\n",
        "threadsperblock = 1024\n",
        "blockspergrid = math.ceil(data.shape[0] / threadsperblock)\n",
        "%timeit my_kernel[blockspergrid, threadsperblock](data)\n",
        "end = time.time()\n",
        "lg_gpu_time = end - start\n",
        "\n",
        "# Plotting\n",
        "\n",
        "plt.bar(['5000'], sm_gpu_time,  width=0.4)\n",
        "plt.bar(['50000000'], lg_gpu_time,  width=0.4)\n",
        "plt.xlabel(\"Array Size\")\n",
        "plt.ylabel(\"Running Time\")\n",
        "plt.title(\"GPU\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# There is a relation between size and performance in both CPU and GPU. For smaller size CPU do better than GPU but as we increase the size the performance of GPU become far better as compare to CPU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "id": "xOCdk-4sNoDB",
        "outputId": "6ec937a5-9c84-4512-bb0f-1ef484be5365"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000 loops, best of 5: 109 µs per loop\n",
            "1 loop, best of 5: 1.09 s per loop\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000 loops, best of 5: 623 µs per loop\n",
            "1 loop, best of 5: 209 ms per loop\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFRUo1qOJNwp"
      },
      "source": [
        "## Exercise 2: Matrix Multiplication\n",
        "\n",
        "In matrix multiplication, every kernel will be reponsible of computing one element of the output matrix. It reads one row from the first matrix (A) and one column form the second matrix (B) and computes the dot product of these two vectors and place it in the corresponding cell in the output matrix (C) as shown in the following figure.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=16EMuj46QLdwKmIDPU0P6AepZ9SNssb2s' width=\"50%\" height=\"50%\"></img>\n",
        "\n",
        "Write a kernel to do the multiplication of two matrices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CZVUq5cSJNwp"
      },
      "outputs": [],
      "source": [
        "# CUDA kernel\n",
        "@cuda.jit\n",
        "def mat_mul(A, B, C):\n",
        "    \"\"\"Perform matrix multiplication of C = A * B\n",
        "    \"\"\"\n",
        "    # get the 2D position of the thread in which it will compute the dot product of the corresponding vectors \n",
        "    row, col = cuda.grid(2)\n",
        "    \n",
        "    if row < C.shape[0] and col < C.shape[1]:\n",
        "        #TODO: Compute the dot product \"prod\" of the corresponding vectors of this position \n",
        "        prod = 0\n",
        "        for idx in range(A.shape[1]):\n",
        "          prod += A[row, idx] * B[row, idx]\n",
        "        C[row, col] = prod\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClcaDBMMJNwp"
      },
      "source": [
        "### Create a host function to invoke the kernel\n",
        "\n",
        "It is a good practice to manually copy the matrices to Device (the GPU memory) using \"cuda.to_device\" to reduce the unnecessary data transfer between the device and the host.\n",
        "\n",
        "\n",
        "To test the kernel \"mat_mul\" we prepare the host function \"gpu_dot\" which will take two matrices as parameters and returns the the output matrix. The job of this host function is to perpare the data and to invoke the kernel.\n",
        "\n",
        "Read the code below and calculate how many blocks are required to start the kernel. Use the calculated values to invoke the kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "cim3RIExJNwq",
        "outputId": "b9d36eb3-8844-4cfc-f2fc-95a3da1fa0dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying Input to GPU time: 0.06344723701477051 s\n",
            "Multiplication Time: 0.27030396461486816 s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CudaAPIError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCudaAPIError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-3309fb517efa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m#Test the host function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpu_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Input Shapes:A:{A.shape}, B:{B.shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-3309fb517efa>\u001b[0m in \u001b[0;36mgpu_dot\u001b[0;34m(A, B)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# Copy the result back to the host\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mstart_copy_back_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC_global_mem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_to_host\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart_copy_back_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Copy result back time: {dt} s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numba/cuda/cudadrv/devices.py\u001b[0m in \u001b[0;36m_require_cuda_context\u001b[0;34m(*args, **kws)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_require_cuda_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_runtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_require_cuda_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numba/cuda/cudadrv/devicearray.py\u001b[0m in \u001b[0;36mcopy_to_host\u001b[0;34m(self, ary, stream)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malloc_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0m_driver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_to_host\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhostary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malloc_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mary\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numba/cuda/cudadrv/driver.py\u001b[0m in \u001b[0;36mdevice_to_host\u001b[0;34m(dst, src, size, stream)\u001b[0m\n\u001b[1;32m   2343\u001b[0m         \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuMemcpyDtoH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2345\u001b[0;31m     \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost_pointer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_pointer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mvarargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numba/cuda/cudadrv/driver.py\u001b[0m in \u001b[0;36msafe_cuda_api_call\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'call driver api: %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0mretcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msafe_cuda_api_call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numba/cuda/cudadrv/driver.py\u001b[0m in \u001b[0;36m_check_error\u001b[0;34m(self, fname, retcode)\u001b[0m\n\u001b[1;32m    335\u001b[0m                     \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_getpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCudaDriverError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CUDA initialized before forking\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCudaAPIError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCudaAPIError\u001b[0m: [700] Call to cuMemcpyDtoH results in UNKNOWN_CUDA_ERROR"
          ]
        }
      ],
      "source": [
        "def gpu_dot(A, B):\n",
        "    #Copy the input matrices to the gpu\n",
        "    start_copy_time = time.time()\n",
        "    A_global_mem = cuda.to_device(A)\n",
        "    B_global_mem = cuda.to_device(B)\n",
        "\n",
        "    # Allocate memory on the device for the result (Note the shape of the output matrix)\n",
        "    C_global_mem = cuda.device_array((A.shape[0], B.shape[1]), np.float32)\n",
        "    \n",
        "    # Configure the blocks\n",
        "    # Specify how many threads per block\n",
        "    threadsperblock = (32, 32)\n",
        "    \n",
        "    #TODO: Calculate how many blocks are required\n",
        "    blockspergrid_x = int(math.ceil(A.shape[0] / threadsperblock[0]))\n",
        "    blockspergrid_y = int(math.ceil(B.shape[1] / threadsperblock[1]))\n",
        "    blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
        "    dt = time.time()-start_copy_time\n",
        "    print(f'Copying Input to GPU time: {dt} s')\n",
        "    start_mult_time = time.time()\n",
        "    \n",
        "    #TODO: Start the kernel based on the calculated grid \n",
        "    mat_mul[blockspergrid, threadsperblock](A_global_mem, B_global_mem, C_global_mem)\n",
        "    \n",
        "    dt = time.time()-start_mult_time\n",
        "    print(f'Multiplication Time: {dt} s')\n",
        "    # Copy the result back to the host\n",
        "    start_copy_back_time = time.time()\n",
        "    C = C_global_mem.copy_to_host()\n",
        "    dt = time.time()-start_copy_back_time\n",
        "    print(f'Copy result back time: {dt} s')\n",
        "    dt = time.time()-start_copy_time\n",
        "    print(f'Total time: {dt} s')\n",
        "    return C\n",
        "\n",
        "# Input Test arrays\n",
        "A = np.full((16384, 2048), 3, np.float32) # matrix containing all 3's\n",
        "B = np.full((2048, 16384), 4, np.float32) # matrix containing all 4's\n",
        "\n",
        "#Test the host function\n",
        "C = gpu_dot(A,B)\n",
        "print(f'Input Shapes:A:{A.shape}, B:{B.shape}')\n",
        "\n",
        "print('Output Shape:', C.shape)\n",
        "print('Output:',C)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48DAlsYdJNwr"
      },
      "source": [
        "### Testing the calculations time compared to numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEjQvFDqJNwr",
        "outputId": "c43bc4c3-b744-4cdd-ce1b-8a08115fe99c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 loop, best of 5: 14.6 s per loop\n"
          ]
        }
      ],
      "source": [
        "%timeit np.dot(A,B)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJMqM-swJNwr"
      },
      "source": [
        "### Comparison between the previous gpu_dot and Numpy.dot\n",
        "- Try different array sizes and compare between CPU (using np.dot) and GPU (using gpu_dot).\n",
        "- Plot a graph that shows the array sizes (bacause it is a 2D matrix, you can consider the size to be the hight x width) on the x axis and the computation time on the y axis of both your kernel and numpy (on the same plot). \n",
        "- Explain what you notice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FTyrwdSJNws"
      },
      "source": [
        "### Exercise 2: Results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Small Size \n",
        "start = time.time()\n",
        "\n",
        "A = np.full((2500, 500), 3, np.float32) # matrix containing all 3's\n",
        "B = np.full((500, 2500), 4, np.float32) # matrix containing all 4's\n",
        "\n",
        "np.dot(A,B)\n",
        "\n",
        "end = time.time()\n",
        "sm_cpu_time = end - start\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "A = np.full((2500, 500), 3, np.float32) # matrix containing all 3's\n",
        "B = np.full((500, 2500), 4, np.float32) # matrix containing all 4's\n",
        "\n",
        "np.dot(A,B)\n",
        "\n",
        "end = time.time()\n",
        "lg_cpu_time = end - start\n",
        "\n",
        "\n",
        "# Plotting for small size data\n",
        "\n",
        "plt.bar(['2500 x 500'], sm_cpu_time,  width=0.4)\n",
        "plt.bar(['25000 x 5000'], lg_cpu_time,  width=0.4)\n",
        "plt.xlabel(\"Size\")\n",
        "plt.ylabel(\"Running Time\")\n",
        "plt.title(\"CPU\")\n",
        "plt.show()\n",
        "\n",
        "# GPU \n",
        "# Small Size\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "A = np.full((2500, 500), 3, np.float32) # matrix containing all 3's\n",
        "B = np.full((500, 2500), 4, np.float32) # matrix containing all 4's\n",
        "\n",
        "gpu_dot(A,B)\n",
        "\n",
        "end = time.time()\n",
        "sm_gpu_time = end - start\n",
        "# Small Size\n",
        "start = time.time()\n",
        "A = np.full((25000, 5000), 3, np.float32) # matrix containing all 3's\n",
        "B = np.full((5000, 25000), 4, np.float32) # matrix containing all 4's\n",
        "\n",
        "gpu_dot(A,B)\n",
        "\n",
        "end = time.time()\n",
        "lg_gpu_time = end - start\n",
        "\n",
        "# Plotting\n",
        "\n",
        "plt.bar(['2500 x 500'], sm_gpu_time,  width=0.4)\n",
        "plt.bar(['25000 x 5000'], lg_gpu_time,  width=0.4)\n",
        "plt.xlabel(\"Size\")\n",
        "plt.ylabel(\"Running Time\")\n",
        "plt.title(\"GPU\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# The gpu_dot performs better than cpu_dot as data's matrices copy to device and calculations done ther before tranfering to host"
      ],
      "metadata": {
        "id": "81KHy0dh9Dvz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        },
        "outputId": "3c9fd173-24ad-4c45-c5fe-25c8ad46ee3d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAUgklEQVR4nO3df7RlZX3f8ffHGRj8CXWYNIZfMxSSrFEqyhSLNQlirUBWGUwhQZsUG5YY6zRJiWlw2UVGVvoDG8EogykNdLGwawElxkx0DElEY5MawmARHHBcI2oYYFUYYRAVZPDbP84eORyee++ZO3efc5l5v9ba6+797Gfv/b0Lzv3M/nGenapCkqRRz5t2AZKkxcmAkCQ1GRCSpCYDQpLUZEBIkpoMCElSkwEhSWoyIKS9kOStSTYneSzJA0k+leR1SdYnebJrfyTJ/0lyUrfN+iQfbeyrkhwz+d9CajMgpHlKcgHwQeA/AX8fOBK4Aljbdbm+ql4ErAD+CvhYkkyjVmk+DAhpHpIcDFwMvKuqPlZV36mqJ6vqT6rqN4f7VtWTwDXAjwLLp1CuNC8GhDQ/JwEHAX80V8cky4C3AfdW1UM91yUtGANCmp/lwENVtWuWPj+f5BHgXuAE4M0TqUxaIEunXYD0HLUDODTJ0llC4oaq+sVG+y7ggOGGJLuXn1zAGqW94hmEND+fB54AzpzHtn8HrBxpW8UgOO7bu7KkhWNASPNQVTuBi4ANSc5M8oIkByQ5Lcn759j8T4GfTPJL3TYvZfAk1B/OcclKmigDQpqnqvoAcAHwH4AHGdxrWAd8fI7tvgmcBrwD+CbwJeAR4J191ivtqfjCIElSi2cQkqQmA0KS1GRASJKaDAhJUtM+80W5Qw89tFauXDntMiTpOeW22257qKpWtNbtMwGxcuVKNm/ePO0yJOk5Jck3ZlrnJSZJUpMBIUlqMiAkSU0GhCSpyYCQJDUZEJKkJgNCktRkQEiSmgwISVLTPvNN6r218sJPTruEfdbX/8vPTrsESfPgGYQkqcmAkCQ1eYlJ0mSsP3jaFey71u/sZbeeQUiSmgwISVKTASFJajIgJElNBoQkqcmAkCQ1GRCSpCYDQpLUZEBIkpoMCElSkwEhSWoyICRJTQaEJKnJgJAkNRkQkqQmA0KS1GRASJKaDAhJUpMBIUlqMiAkSU29BkSSU5NsTbItyYWN9cuSXN+tvyXJyq79gCTXJLkzyd1J3tNnnZKkZ+stIJIsATYApwGrgbckWT3S7Tzg4ao6BrgMuKRrPxtYVlXHAScA79gdHpKkyejzDOJEYFtV3VNV3weuA9aO9FkLXNPN3wi8IUmAAl6YZCnwfOD7wKM91ipJGtFnQBwG3Du0vL1ra/apql3ATmA5g7D4DvAA8HfA71bVt0YPkOT8JJuTbH7wwQcX/jeQpP3YYr1JfSLwFPBjwCrgN5IcPdqpqq6sqjVVtWbFihWTrlGS9ml9BsR9wBFDy4d3bc0+3eWkg4EdwFuBP62qJ6vqm8BfA2t6rFWSNKLPgLgVODbJqiQHAucAG0f6bATO7ebPAm6uqmJwWekUgCQvBP4x8OUea5UkjegtILp7CuuAm4C7gRuqakuSi5Oc0XW7ClieZBtwAbD7UdgNwIuSbGEQNP+jqu7oq1ZJ0rMt7XPnVbUJ2DTSdtHQ/OMMHmkd3e6xVrskaXIW601qSdKUGRCSpCYDQpLUZEBIkpoMCElSkwEhSWoyICRJTQaEJKnJgJAkNRkQkqQmA0KS1GRASJKaDAhJUpMBIUlqMiAkSU0GhCSpyYCQJDUZEJKkJgNCktRkQEiSmgwISVKTASFJajIgJElNBoQkqcmAkCQ1GRCSpCYDQpLUZEBIkpoMCElSkwEhSWoyICRJTQaEJKlp7IBI8oI+C5EkLS5zBkSS1ya5C/hyt/zKJFf0XpkkaarGOYO4DHgTsAOgqr4I/HSfRUmSpm+sS0xVde9I01M91CJJWkTGCYh7k7wWqCQHJHk3cPc4O09yapKtSbYlubCxflmS67v1tyRZObTuHyb5fJItSe5MctCYv5MkaQGMExC/ArwLOAy4Dzi+W55VkiXABuA0YDXwliSrR7qdBzxcVccwuJR1SbftUuCjwK9U1cuBk4Enx6hVkrRAls7VoaoeAv7lPPZ9IrCtqu4BSHIdsBa4a6jPWmB9N38jcHmSAP8MuKO730FV7ZjH8SVJe2HOgEiyCvi3wMrh/lV1xhybHgYM37vYDrxmpj5VtSvJTmA58OMMLmndBKwArquq9zdqOx84H+DII4+c61eRJO2BOQMC+DhwFfAnwA/6LeeHlgKvA/4R8F3g00luq6pPD3eqqiuBKwHWrFlTE6pNkvYL4wTE41X1oXns+z7giKHlw7u2Vp/t3X2Hgxk8Trsd+Fx3eYskm4BXA59GkjQR49yk/r0kv53kpCSv3j2Nsd2twLFJViU5EDgH2DjSZyNwbjd/FnBzVRVwE3Bckhd0wfEzPPPehSSpZ+OcQRwH/BJwCk9fYqpueUbdPYV1DP7YLwGurqotSS4GNlfVRgaXrq5Nsg34FoMQoaoeTnIpg5ApYFNVfXKPfztJ0ryNExBnA0dX1ff3dOdVtQnYNNJ20dD8493+W9t+lMGjrpKkKRjnEtOXgEP6LkSStLiMcwZxCPDlJLcCT+xuHOMxV0nSc9g4AfHbvVchSVp0xvkm9V9OohBJ0uIyY0Ak+auqel2SbzN4kuiHq4Cqqpf0Xp0kaWpmO4P4TYCqevGEapEkLSKzPcW0YWJVSJIWndkCIhOrQpK06Mx2iWlVktGhMX7Ix1wlad82W0A8CHxgUoVIkhaX2QLi2z7iKkn7r9nuQXx9UkVIkhafGQOiqn5ukoVIkhaXcQbrkyTthwwISVLTnGMxzfD2uJ3AN6pq18KXJElaDMYZzfUKBu+DvoPBl+deAWwBDk7yzqr6sx7rkyRNyTiXmO4HXlVVa6rqBOBVwD3AG4H391mcJGl6xgmIH6+qLbsXquou4Cer6p7+ypIkTds4l5i2JPkIcF23/AvAXUmWAU/2VpkkaarGOYN4G7AN+PVuuqdrexJ4fV+FSZKma5w3yn2PwZhMrXGZHlvwiiRJi8I4j7n+E2A9cNRw/6o6ur+yJEnTNs49iKuAfwfcBjzVbzmSpMVinIDYWVWf6r0SSdKiMk5AfCbJfwU+Bjyxu7GqvtBbVZKkqRsnIF7T/Vwz1FbAKQtfjiRpsRjnKSYfZZWk/dCMAZHkF6vqo0kuaK2vqkv7K0uSNG2znUG8sPv54kkUIklaXGYMiKr6b93P902uHEnSYjHOF+VWAG8HVvLML8r9cn9lSZKmbZynmP4Y+N/AX+AX5SRpvzFOQLygqn6r90okSYvKOKO5fiLJ6b1XIklaVMYJiF9jEBLfS/Jokm8nebTvwiRJ0zVnQFTVi6vqeVX1/Kp6Sbf8knF2nuTUJFuTbEtyYWP9siTXd+tvSbJyZP2RSR5L8u5xfyFJ0sIY5x4ESQ7j2cN9f26ObZYAGxi8u3o7cGuSjd0rS3c7D3i4qo5Jcg5wCYM31u12KeBAgZI0BeM85rr7j/ZdPP0UUwGzBgRwIrBt97urk1wHrO32s9taBu+aALgRuDxJqqqSnAl8DfjOeL+KJGkhjXMGcSbwE1X1xJw9n+kw4N6h5e08PfDfs/pU1a4kO4HlSR4HfovB2ceMl5eSnA+cD3DkkUfuYXmSpNmMc5P6HuCAvgsZsR64rKpmfaVpVV1ZVWuqas2KFSsmU5kk7SfGOYP4LnB7kk/zzPdB/Ooc290HHDG0fHjX1uqzPclS4GBgB4MzjbOSvB84BPhBkser6vIx6pUkLYBxAmJjN+2pW4Fjk6xiEATnAG9t7Ptc4PPAWcDNVVXAT+3ukGQ98JjhIEmTNc77IK6Zz467ewrrgJuAJcDVVbUlycXA5qrayOB919cm2QZ8i0GISJIWgXGeYvoag6eWnqGqjp5r26raBGwaabtoaP5x4Ow59rF+ruNIkhbeOJeYhl81ehCDP+gv7accSdJiMc43qXcMTfdV1QeBn51AbZKkKRrnEtOrhxafx+CMYqxvYEuSnrvG+UP/gaH5XcDXmeO+gSTpuW+cp5heP7zcjbF0DvCVvoqSJE3fjPcgkrwkyXuSXJ7kjRlYB2wDfn5yJUqSpmG2M4hrgYcZfInt7cB7gQBvrqrbJ1CbJGmKZguIo6vqOIAkfwA8ABzZfXdBkrSPm+0x1yd3z1TVU8B2w0GS9h+znUG8cujVogGe3y0HqHHfKidJem6aMSCqaskkC5EkLS7jvA9CkrQfMiAkSU0GhCSpyYCQJDUZEJKkJgNCktRkQEiSmgwISVKTASFJajIgJElNBoQkqcmAkCQ1GRCSpCYDQpLUZEBIkpoMCElSkwEhSWoyICRJTQaEJKnJgJAkNRkQkqQmA0KS1GRASJKaDAhJUpMBIUlq6jUgkpyaZGuSbUkubKxfluT6bv0tSVZ27W9McluSO7ufp/RZpyTp2XoLiCRLgA3AacBq4C1JVo90Ow94uKqOAS4DLunaHwL+eVUdB5wLXNtXnZKktj7PIE4EtlXVPVX1feA6YO1In7XANd38jcAbkqSq/m9V3d+1bwGen2RZj7VKkkb0GRCHAfcOLW/v2pp9qmoXsBNYPtLnXwBfqKonRg+Q5Pwkm5NsfvDBBxescEnSIr9JneTlDC47vaO1vqqurKo1VbVmxYoVky1OkvZxfQbEfcARQ8uHd23NPkmWAgcDO7rlw4E/Av5VVX21xzolSQ19BsStwLFJViU5EDgH2DjSZyODm9AAZwE3V1UlOQT4JHBhVf11jzVKkmbQW0B09xTWATcBdwM3VNWWJBcnOaPrdhWwPMk24AJg96Ow64BjgIuS3N5NP9JXrZKkZ1va586rahOwaaTtoqH5x4GzG9v9DvA7fdYmSZrdor5JLUmaHgNCktRkQEiSmgwISVKTASFJajIgJElNBoQkqcmAkCQ1GRCSpCYDQpLUZEBIkpoMCElSkwEhSWoyICRJTQaEJKnJgJAkNRkQkqQmA0KS1GRASJKaDAhJUpMBIUlqMiAkSU0GhCSpyYCQJDUZEJKkJgNCktRkQEiSmgwISVKTASFJajIgJElNBoQkqcmAkCQ1GRCSpCYDQpLUZEBIkpoMCElSU68BkeTUJFuTbEtyYWP9siTXd+tvSbJyaN17uvatSd7UZ52SpGfrLSCSLAE2AKcBq4G3JFk90u084OGqOga4DLik23Y1cA7wcuBU4Ipuf5KkCenzDOJEYFtV3VNV3weuA9aO9FkLXNPN3wi8IUm69uuq6omq+hqwrdufJGlClva478OAe4eWtwOvmalPVe1KshNY3rX/zci2h40eIMn5wPnd4mNJti5M6YveocBD0y5iXLlk2hVIe+w59RnjfdmbrY+aaUWfAdG7qroSuHLadUxaks1VtWbadUj7Kj9jA31eYroPOGJo+fCurdknyVLgYGDHmNtKknrUZ0DcChybZFWSAxncdN440mcjcG43fxZwc1VV135O95TTKuBY4G97rFWSNKK3S0zdPYV1wE3AEuDqqtqS5GJgc1VtBK4Crk2yDfgWgxCh63cDcBewC3hXVT3VV63PQfvdZTVpwvyMARn8g12SpGfym9SSpCYDQpLUZEAsgCRHJPlMkruSbEnya0Pr1ie5L8nt3XT60LrmcCJzDVGyB3VN7djSfEzqs9Q9PHNL13599yDNfOo9OcnOoZoumu+xZxt6aGqqymkvJ+BlwKu7+RcDXwFWd8vrgXc3tlkNfBFYBqwCvsrgZv6Sbv5o4MCuz+p51jW1Yzs5zWea1GcJuAE4p5v/feCd86z3ZOATjfY9Pjbwb4Df7+bPAa6f9n8PzyAWQFU9UFVf6Oa/DdxN45vfI2YaTmTOIUqSLE1ya5KTu+X/nOQ/7kHJ8z621KdJfJa64XxOYTC8DwyG+zlzdKdJfm/3GUGSNyX5XJJx/2bO59gzDT00NQbEAutOC18F3DLUvC7JHUmuTvL3urbWUCSHzdL+Q1W1C3gb8JEk/5TBgIbvm6GkBT22NCk9fpaWA490n6Ph9lHvAX4hyeuBDwH/uqp+0Oh3UpIvJvlUkpfPUdNsx37G0EPA7qGHpsaAWEBJXgT8IfDrVfVo1/wR4B8AxwMPAB9YiGNV1RbgWuATwC93/0oZ1cuxpb5N8rM0k6r6LvB24M+By6vqq41uXwCOqqpXAh8GPt5nTZNmQCyQJAcw+B/6f1bVx3a3V9X/q6qnun95/HeeHpV2puFE9mSYkeOAR4Afaa3s+dhSLybwWdoBHNIN7zPc3nJc1//HWiur6tGqeqyb3wQckOTQeR57pqGHpsaAWADddcKrgLur6tKRdS8bWnwz8KVufqbhRMYZooQkPwe8FPhp4MNJDmn06eXYUl8m8VmqwV3gzzAY3gcGw/38caOWo4DfYHCZ67Qko6NRk+RHd98nSHIig7+pO+Z57JmGHpqead8l3xcm4HVAAXcAt3fT6d26a4E7u3UbgZcNbfdeBk86bAVOG2o/ncHTG18F3ts43qHd+iO65V8Frmn0W/BjOzn1OU3qs8Tg6aK/ZXBD+38By0bqCPAXwBnd8gndsQ8a6bcO2MLgKaW/AV4732MDB3XL27r1R0/7v4dDbUiSmrzEJElqMiAkSU0GhCSpyYCQJDUZEJKkJgNC2ktJ3tuNPHpHN6Lna5L8QZLV065N2hs+5irthSQnAZcCJ1fVE923aA+sqvunXJq01zyDkPbOy4CHquoJgKp6qKruT/LZJGuSnDH0roCtSb4GkOSEJH+Z5LYkN418S1haFAwIae/8GXBEkq8kuSLJzwyvrKqNVXV8VR3P4Nu2v9uNNfRh4KyqOgG4GtiT4dqliVg6dxdJM6mqx5KcAPwU8Hrg+jTexJfk3wPfq6oNSV4BvAL4824YnyUMRieVFhUDQtpLVfUU8Fngs0nu5OkB1wDo3tlxNoOBFWEwzs+WqjppknVKe8pLTNJeSPITSY4dajoe+MbQ+qOADcDZVfW9rnkrsKK7wU2SA4ZeNCMtGp5BSHvnRTw93PouBiNxns/Tr5R8G4O3gn28u5x0f1WdnuQs4ENJDmbwOfwgg1FBpUXDx1wlSU1eYpIkNRkQkqQmA0KS1GRASJKaDAhJUpMBIUlqMiAkSU3/HxfemvYGMFkvAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "error",
          "ename": "CudaAPIError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCudaAPIError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-a7debedaf42c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# matrix containing all 4's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mgpu_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-3309fb517efa>\u001b[0m in \u001b[0;36mgpu_dot\u001b[0;34m(A, B)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m#Copy the input matrices to the gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstart_copy_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mA_global_mem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mB_global_mem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numba/cuda/cudadrv/devices.py\u001b[0m in \u001b[0;36m_require_cuda_context\u001b[0;34m(*args, **kws)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_require_cuda_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_runtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_require_cuda_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numba/cuda/api.py\u001b[0m in \u001b[0;36mto_device\u001b[0;34m(obj, stream, copy, to)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \"\"\"\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mto\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevicearray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numba/cuda/cudadrv/devicearray.py\u001b[0m in \u001b[0;36mauto_device\u001b[0;34m(obj, stream, copy)\u001b[0m\n\u001b[1;32m    764\u001b[0m                 subok=True)\n\u001b[1;32m    765\u001b[0m             \u001b[0msentry_contiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m             \u001b[0mdevobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrom_array_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0mdevobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numba/cuda/cudadrv/devicearray.py\u001b[0m in \u001b[0;36mfrom_array_like\u001b[0;34m(ary, stream, gpu_data)\u001b[0m\n\u001b[1;32m    686\u001b[0m     \u001b[0;34m\"Create a DeviceNDArray object that is like ary.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     return DeviceNDArray(ary.shape, ary.strides, ary.dtype,\n\u001b[0;32m--> 688\u001b[0;31m                          writeback=ary, stream=stream, gpu_data=gpu_data)\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numba/cuda/cudadrv/devicearray.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, shape, strides, dtype, stream, writeback, gpu_data)\u001b[0m\n\u001b[1;32m    102\u001b[0m                                                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                                                                 self.dtype.itemsize)\n\u001b[0;32m--> 104\u001b[0;31m                 \u001b[0mgpu_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemalloc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malloc_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malloc_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_driver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_memory_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numba/cuda/cudadrv/driver.py\u001b[0m in \u001b[0;36mmemalloc\u001b[0;34m(self, bytesize)\u001b[0m\n\u001b[1;32m   1099\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmemalloc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytesize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemalloc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytesize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmemhostalloc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytesize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapped\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mportable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numba/cuda/cudadrv/driver.py\u001b[0m in \u001b[0;36mmemalloc\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    849\u001b[0m             \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuMemAlloc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attempt_allocation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallocator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m         \u001b[0mfinalizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_alloc_finalizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numba/cuda/cudadrv/driver.py\u001b[0m in \u001b[0;36m_attempt_allocation\u001b[0;34m(self, allocator)\u001b[0m\n\u001b[1;32m    709\u001b[0m         \"\"\"\n\u001b[1;32m    710\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m             \u001b[0mallocator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCudaAPIError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0;31m# is out-of-memory?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numba/cuda/cudadrv/driver.py\u001b[0m in \u001b[0;36mallocator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mallocator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m             \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuMemAlloc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attempt_allocation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallocator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numba/cuda/cudadrv/driver.py\u001b[0m in \u001b[0;36msafe_cuda_api_call\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'call driver api: %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0mretcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msafe_cuda_api_call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numba/cuda/cudadrv/driver.py\u001b[0m in \u001b[0;36m_check_error\u001b[0;34m(self, fname, retcode)\u001b[0m\n\u001b[1;32m    335\u001b[0m                     \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_getpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCudaDriverError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CUDA initialized before forking\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCudaAPIError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCudaAPIError\u001b[0m: [700] Call to cuMemAlloc results in UNKNOWN_CUDA_ERROR"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6fhf9lMJNws"
      },
      "source": [
        "## Exercise 3: Distance Matrix\n",
        "The distance matrix (D) of a data matrix (A) is the matrix that contains the eucleadian distance between each two row vectors as shown in the following figure.\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1UMMRYmtPW9_Tonq20GBjxsDLrNFYSTdc' width=\"50%\" height=\"50%\"></img>\n",
        "\n",
        "where \n",
        "$$D[i,j]=D[j,i]=dist(A[i,:], A[j,:])$$\n",
        "\n",
        "\n",
        "Use what you have learned in the previous exercises to write a kernel and a host function to compute the distance matrix of a data matrix. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NWoMKJj-JNws",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "cc4e9520-0871-4876-b394-a2911c38edcf"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "CudaAPIError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCudaAPIError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-de06726fe931>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpu_dist_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-de06726fe931>\u001b[0m in \u001b[0;36mgpu_dist_matrix\u001b[0;34m(mat)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mgrid_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0minMatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0moutMatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numba/cuda/cudadrv/devices.py\u001b[0m in \u001b[0;36m_require_cuda_context\u001b[0;34m(*args, **kws)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_require_cuda_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_runtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_require_cuda_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numba/cuda/api.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m()\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0mCreate\u001b[0m \u001b[0ma\u001b[0m \u001b[0mCUDA\u001b[0m \u001b[0mstream\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mrepresents\u001b[0m \u001b[0ma\u001b[0m \u001b[0mcommand\u001b[0m \u001b[0mqueue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \"\"\"\n\u001b[0;32m--> 269\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcurrent_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mrequire_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numba/cuda/cudadrv/driver.py\u001b[0m in \u001b[0;36mcreate_stream\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1172\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m         \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdrvapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcu_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m         \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuStreamCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1175\u001b[0m         return Stream(weakref.proxy(self), handle,\n\u001b[1;32m   1176\u001b[0m                       _stream_finalizer(self.deallocations, handle))\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numba/cuda/cudadrv/driver.py\u001b[0m in \u001b[0;36msafe_cuda_api_call\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'call driver api: %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0mretcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msafe_cuda_api_call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numba/cuda/cudadrv/driver.py\u001b[0m in \u001b[0;36m_check_error\u001b[0;34m(self, fname, retcode)\u001b[0m\n\u001b[1;32m    335\u001b[0m                     \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_getpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCudaDriverError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CUDA initialized before forking\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCudaAPIError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCudaAPIError\u001b[0m: [700] Call to cuStreamCreate results in UNKNOWN_CUDA_ERROR"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from numba import cuda\n",
        "\n",
        "@cuda.jit()\n",
        "def distance_matrix(mat, out):\n",
        "    #TODO: write a kernel to compute the distance matrix of the input \"mat\" and place the result in \"out\"\n",
        "    row, column = cuda.grid(2)\n",
        "\n",
        "    distance = 0\n",
        "    if row < mat.shape[0] and column < mat.shape[1]:\n",
        "        for i in range(mat.shape[1]):\n",
        "            distance +=(mat[row, i] - mat[column, i])**2\n",
        "\n",
        "        out[row, column] = math.sqrt(distance)\n",
        "\n",
        "def gpu_dist_matrix(mat):\n",
        "    #TODO: write a host function to calculate the grid size and use the calculated values to invoke the \"distance_Matrix\" kernel\n",
        "    row = mat.shape[0]\n",
        "    column = mat.shape[1]\n",
        "\n",
        "    blocks = 32\n",
        "    grid_dim = (int(row/blocks), int(column/blocks))\n",
        "  \n",
        "    stream = cuda.stream()\n",
        "    inMatrix = cuda.to_device(np.asarray(mat)) \n",
        "    outMatrix = cuda.device_array((row, column)) \n",
        "    distance_matrix[grid_dim, (blocks, blocks)](inMatrix, outMatrix) \n",
        "    outMatrix = outMatrix.copy_to_host(stream=stream)\n",
        "   \n",
        "    return outMatrix\n",
        "\n",
        "\n",
        "A = np.random.randn(1024,1024)\n",
        "D = gpu_dist_matrix(A)\n",
        "print(D)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKdNMS4tJNwt"
      },
      "source": [
        "# Homework: K-Nearest Neighbors (GPU version)\n",
        "\n",
        "K-Nearest Neighbors is one of the simplest and most intuitive algorithms in machine learning that relies on the principle that close points behave similarly. It is one of the case-based learning algorithms that can learn non-linear complicated decision boundaries with a single hyperparameter, i.e. K the number of nearest neighbors. The problem of this algorithm is that, to find the k nearest neighbors of a specific point, you have to compute the distances to all the points in the training dataset, which is very costly in terms of computation especially with a large amount of data. A great benefit can be achieved by performing such computation on the GPU.\n",
        "\n",
        "Your task is to implement the K-Nearest Neighbors algorithm using python, and Numba, and CUDA programming.\n",
        "Identify the parts of the algorithm that can make use of the GPU and implement them as CUDA kernels.\n",
        "\n",
        "Use the MNIST dataset as an example and implement a K-Nearest Neighbors classifier to classify the image of the digit into its category.\n",
        "\n",
        "Try different numbers of K and figure out the number that maximizes the accuracy of the classifier.\n",
        "Build another K-Nearest Neighbors using the Sciket-learn library and compare the computation time with your GPU-enabled algorithm. \n",
        "\n",
        "You can download MNIST from Keras library: ( https://keras.io/api/datasets/mnist/ )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAUocKI-JNwu"
      },
      "source": [
        "### Homework: Reported Time and Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1dTnQE2DJNwu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05f66d65-25ac-48d5-9494-23b6760551c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# GPU-enabled KNNNeighbour\n",
        "import tensorflow as tf\n",
        "from collections import Counter\n",
        "\n",
        "(X_train, y_train),(X_test, y_test) = tf.keras.datasets.mnist.load_data(path='mnist.npz')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def knn(k, X_test):\n",
        "  output = []\n",
        "  for i in range(len(X_test)):\n",
        "    distances = []\n",
        "    points = []\n",
        "    for j in range(len(X_train)):\n",
        "      distances.append([gpu_dist_matrix(np.array((X_train[j],X_test[i]))), j])\n",
        "      \n",
        "    distances.sort()\n",
        "    distances = distances[0:k]\n",
        "    for d, j in distances:\n",
        "        points.append(y_train[j])\n",
        "        ans = Counter(points).most_common(1)[0][0]\n",
        "        output.append(ans)\n",
        "\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "pztOTkFNF77F"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_knn = knn(1, X_test)\n",
        "accuracy = (gpu_knn == y_test).sum() / len(y_test)\n",
        "print(f\"K = 1 : {accuracy}\")\n",
        "\n",
        "gpu_knn = knn(5, X_test)\n",
        "accuracy = (gpu_knn == y_test).sum() / len(y_test)\n",
        "print(f\"K = 5 : {accuracy}\")\n",
        "\n",
        "gpu_knn = knn(25, X_test)\n",
        "accuracy = (gpu_knn == y_test).sum() / len(y_test)\n",
        "print(f\"K = 25 : {accuracy}\")"
      ],
      "metadata": {
        "id": "lkHg0iAnZ9xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN using Sklearn\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "mnist_dataset = datasets.load_digits()\n",
        "\n",
        "(X_train, X_test, y_train, y_test) = train_test_split(np.array(mnist_dataset.data), mnist_dataset.target, test_size=0.25, random_state=42)\n",
        "\n",
        "clf = KNeighborsClassifier(n_neighbors= 1)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "test_score = clf.score(X_test, y_test)\n",
        "\n",
        "print(f\"K = 1 : {test_score}\")\n",
        "\n",
        "clf = KNeighborsClassifier(n_neighbors= 5)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "test_score = clf.score(X_test, y_test)\n",
        "\n",
        "print(f\"K = 5 : {test_score}\")\n",
        "\n",
        "clf = KNeighborsClassifier(n_neighbors= 25)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "test_score = clf.score(X_test, y_test)\n",
        "\n",
        "print(f\"K = 25 : {test_score}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bDhqj2MbDO_",
        "outputId": "6d9ca92b-c269-434b-c43a-a38a20e7cdd7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K = 1 : 0.9822222222222222\n",
            "K = 5 : 0.9933333333333333\n",
            "K = 25 : 0.9733333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The lower the value of K, the more it'll be accurate. \n",
        "# The Computation time of GPU enabled one is lower than the Sklearn"
      ],
      "metadata": {
        "id": "Kk-YV33EkB_W"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Ej-naGmInLy7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "name": "Lab4_StudentVersion(1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}